{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Url: https://tbrain.trendmicro.com.tw/Competitions/Details/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, udf, lag, rank, lit\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global Path\n",
    "if sc.master[0:5]==\"local\":\n",
    "    Path = \"file:/c:/D Drive/work/bigData/pySpark/TBrain_Round2_DataSet_20180427\"\n",
    "    #Path = \"file:/Users/yungchuanlee/Documents/learn/AI競賽/ETF預測/TBrain_Round2_DataSet_20180427\"\n",
    "else:\n",
    "    Path = \"hdfs://master:9000/user/hduser\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29066.57"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(\"    46.57\")+float(\"     29,020\".replace(\",\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define alias of columns\n",
    "col_alias_etf= {\"代碼\":\"etf_id\", \"日期\": \"etf_date\", \"中文簡稱\": \"etf_name\", \"開盤價(元)\":\"etf_open\", \n",
    "            \"最高價(元)\":\"etf_high\", \"最低價(元)\":\"etf_low\", \"收盤價(元)\":\"etf_close\", \"成交張數(張)\":\"etf_count\"}\n",
    "col_alias_stock= {\"代碼\":\"stock_id\", \"日期\": \"stock_date\", \"中文簡稱\": \"stock_name\", \"開盤價(元)\":\"stock_open\", \n",
    "            \"最高價(元)\":\"stock_high\", \"最低價(元)\":\"stock_low\", \"收盤價(元)\":\"stock_close\", \"成交張數(張)\":\"stock_count\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#udf\n",
    "def to_double(str_val):\n",
    "    return float(str_val.replace(\",\",\"\"))\n",
    "to_double=udf(to_double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def function to read data (因檔案格式都相同)\n",
    "def read_data(file_name, col_alias):\n",
    "    str_cols = [\"代碼\",\"日期\", \"中文簡稱\"]\n",
    "    raw_data = spark.read.option(\"encoding\", \"Big5\").csv(Path + \"/\" + file_name, header=True, sep=\",\")\n",
    "    print(\"Total \" + file_name + \" count: \" + str(raw_data.count()))\n",
    "    #rename cols and correct type \n",
    "    num_cols = [col_name for col_name in raw_data.columns if col_name not in str_cols]\n",
    "    final_data=raw_data.select( [col(str_col_name).alias(col_alias[str_col_name]) for str_col_name in str_cols] + \n",
    "                                  [to_double(col(num_col_name)).cast(\"double\").alias(col_alias[num_col_name]) for num_col_name in num_cols] )\n",
    "    final_data.printSchema()\n",
    "    final_data.show(5)\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting import tetfp.csv(台灣18檔ETF股價資料)...\n",
      "Total tetfp.csv count: 19054\n",
      "root\n",
      " |-- etf_id: string (nullable = true)\n",
      " |-- etf_date: string (nullable = true)\n",
      " |-- etf_name: string (nullable = true)\n",
      " |-- etf_open: double (nullable = true)\n",
      " |-- etf_high: double (nullable = true)\n",
      " |-- etf_low: double (nullable = true)\n",
      " |-- etf_close: double (nullable = true)\n",
      " |-- etf_count: double (nullable = true)\n",
      "\n",
      "+-------+--------+----------------+--------+--------+-------+---------+---------+\n",
      "| etf_id|etf_date|        etf_name|etf_open|etf_high|etf_low|etf_close|etf_count|\n",
      "+-------+--------+----------------+--------+--------+-------+---------+---------+\n",
      "|0050   |20130102|元大台灣50          |    54.0|   54.65|   53.9|     54.4|  16487.0|\n",
      "|0050   |20130103|元大台灣50          |    54.9|   55.05|  54.65|    54.85|  29020.0|\n",
      "|0050   |20130104|元大台灣50          |   54.85|   54.85|   54.4|     54.5|   9837.0|\n",
      "|0050   |20130107|元大台灣50          |   54.55|   54.55|   53.9|    54.25|   8910.0|\n",
      "|0050   |20130108|元大台灣50          |    54.0|    54.2|  53.65|     53.9|  12507.0|\n",
      "+-------+--------+----------------+--------+--------+-------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"starting import tetfp.csv(台灣18檔ETF股價資料)...\")\n",
    "tetfp_dt=read_data(\"tetfp.csv\", col_alias_etf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|            etf_date|\n",
      "+-------+--------------------+\n",
      "|  count|               19054|\n",
      "|   mean|2.0153294704733916E7|\n",
      "| stddev|  15718.272009667517|\n",
      "|    min|            20130102|\n",
      "|    max|            20180427|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#EDA\n",
    "#range of date\n",
    "tetfp_dt.describe('etf_date').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting import taetfp.csv(台灣18檔ETF調整後股價資料)...\n",
      "Total taetfp.csv count: 19053\n",
      "root\n",
      " |-- etf_id: string (nullable = true)\n",
      " |-- etf_date: string (nullable = true)\n",
      " |-- etf_name: string (nullable = true)\n",
      " |-- etf_open: double (nullable = true)\n",
      " |-- etf_high: double (nullable = true)\n",
      " |-- etf_low: double (nullable = true)\n",
      " |-- etf_close: double (nullable = true)\n",
      " |-- etf_count: double (nullable = true)\n",
      "\n",
      "+-------+--------+----------------+--------+--------+-------+---------+---------+\n",
      "| etf_id|etf_date|        etf_name|etf_open|etf_high|etf_low|etf_close|etf_count|\n",
      "+-------+--------+----------------+--------+--------+-------+---------+---------+\n",
      "|0050   |20130102|元大台灣50          |   46.57|   47.13|  46.49|    46.92|  16487.0|\n",
      "|0050   |20130103|元大台灣50          |   47.35|   47.48|  47.13|    47.31|  29020.0|\n",
      "|0050   |20130104|元大台灣50          |   47.31|   47.31|  46.92|     47.0|   9837.0|\n",
      "|0050   |20130107|元大台灣50          |   47.05|   47.05|  46.49|    46.79|   8910.0|\n",
      "|0050   |20130108|元大台灣50          |   46.57|   46.75|  46.27|    46.49|  12507.0|\n",
      "+-------+--------+----------------+--------+--------+-------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"starting import taetfp.csv(台灣18檔ETF調整後股價資料)...\")\n",
    "taetfp_dt=read_data(\"taetfp.csv\", col_alias_etf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting import tsharep.csv(台灣個股股價資料)...\n",
      "Total tsharep.csv count: 2012609\n",
      "root\n",
      " |-- stock_id: string (nullable = true)\n",
      " |-- stock_date: string (nullable = true)\n",
      " |-- stock_name: string (nullable = true)\n",
      " |-- stock_open: double (nullable = true)\n",
      " |-- stock_high: double (nullable = true)\n",
      " |-- stock_low: double (nullable = true)\n",
      " |-- stock_close: double (nullable = true)\n",
      " |-- stock_count: double (nullable = true)\n",
      "\n",
      "+--------+----------+------------------+----------+----------+---------+-----------+-----------+\n",
      "|stock_id|stock_date|        stock_name|stock_open|stock_high|stock_low|stock_close|stock_count|\n",
      "+--------+----------+------------------+----------+----------+---------+-----------+-----------+\n",
      "| 1101   |  20130102|台泥                |     38.95|      39.1|    38.65|       39.0|     6374.0|\n",
      "| 1101   |  20130103|台泥                |      39.5|      39.5|    38.75|      38.85|     9710.0|\n",
      "| 1101   |  20130104|台泥                |      39.4|     39.45|     38.6|       39.0|     8682.0|\n",
      "| 1101   |  20130107|台泥                |      39.1|      39.1|    38.65|       38.9|     5067.0|\n",
      "| 1101   |  20130108|台泥                |      38.9|      39.1|     38.2|       38.5|     6454.0|\n",
      "+--------+----------+------------------+----------+----------+---------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"starting import tsharep.csv(台灣個股股價資料)...\")\n",
    "tsharep_dt=read_data(\"tsharep.csv\", col_alias_stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting import tasharep.csv(台灣個股調整後股價資料)...\n",
      "Total tasharep.csv count: 2012609\n",
      "root\n",
      " |-- stock_id: string (nullable = true)\n",
      " |-- stock_date: string (nullable = true)\n",
      " |-- stock_name: string (nullable = true)\n",
      " |-- stock_open: double (nullable = true)\n",
      " |-- stock_high: double (nullable = true)\n",
      " |-- stock_low: double (nullable = true)\n",
      " |-- stock_close: double (nullable = true)\n",
      " |-- stock_count: double (nullable = true)\n",
      "\n",
      "+--------+----------+------------------+----------+----------+---------+-----------+-----------+\n",
      "|stock_id|stock_date|        stock_name|stock_open|stock_high|stock_low|stock_close|stock_count|\n",
      "+--------+----------+------------------+----------+----------+---------+-----------+-----------+\n",
      "| 1101   |  20130102|台泥                |     30.41|     30.53|    30.18|      30.45|     6374.0|\n",
      "| 1101   |  20130103|台泥                |     30.84|     30.84|    30.25|      30.33|     9710.0|\n",
      "| 1101   |  20130104|台泥                |     30.76|      30.8|    30.14|      30.45|     8682.0|\n",
      "| 1101   |  20130107|台泥                |     30.53|     30.53|    30.18|      30.37|     5067.0|\n",
      "| 1101   |  20130108|台泥                |     30.37|     30.53|    29.82|      30.06|     6454.0|\n",
      "+--------+----------+------------------+----------+----------+---------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"starting import tasharep.csv(台灣個股調整後股價資料)...\")\n",
    "tasharep_dt=read_data(\"tasharep.csv\", col_alias_stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql.functions import lag, col, avg,collect_list, lit\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "#declare previous row windows\n",
    "wsSpec_etf = Window.partitionBy('etf_id').orderBy('etf_date') #time window for normal case\n",
    "wsSpec_etf_close_price_raw = Window.partitionBy('etf_id').orderBy('row_idx').rangeBetween(-sys.maxsize, -1)\n",
    "wsSpec_etf_dif_raw = Window.partitionBy('etf_id').orderBy('row_idx').rangeBetween(-sys.maxsize, 0)\n",
    "def avg_list(p_list):\n",
    "    #計算數字list的平均值\n",
    "    return sum(p_list)/len(p_list)\n",
    "#計算EMA的udf\n",
    "def calculate_ema_native(close_p_list, window_len):\n",
    "    #透過歷史收盤價計算\n",
    "    if len(close_p_list) < window_len:\n",
    "        return None\n",
    "    elif len(close_p_list) == window_len:\n",
    "        #if len of list = win_len then return avg, \n",
    "        return avg_list(close_p_list)\n",
    "    else:\n",
    "        #else EMA[t] =(EMA[t-1]*(win_len-1)+close[t]*2)/(win_len+1)\n",
    "        ema = avg_list(close_p_list[:window_len])\n",
    "        for price in close_p_list[window_len:]:\n",
    "            ema = (ema*(window_len-1)+price*2)/(window_len+1)\n",
    "        return ema\n",
    "calculate_ema=udf(calculate_ema_native, DoubleType())\n",
    "#計算BIAS的udf\n",
    "def calculate_bias(close_p_list):\n",
    "    #計算前日收盤價與N日均線之差比: (close price - MA)/MA   ,Paper 建議用20日MA\n",
    "    #因要預測今日的收盤價，故計算前日收盤價與前20日均線\n",
    "    if len(close_p_list) < 21:\n",
    "        return None\n",
    "    else:\n",
    "        list_len = len(close_p_list)\n",
    "        p_close = close_p_list[-1]\n",
    "        cal_list = close_p_list[list_len-21: list_len-1]\n",
    "        return p_close - avg_list(cal_list)\n",
    "calculate_bias=udf(calculate_bias, DoubleType())\n",
    "\n",
    "def get_min_max_last(p_list):\n",
    "    #找出list中最大最小和最後一個值, 回傳(min, max, last)\n",
    "    return (min(p_list), max(p_list), p_list[-1])\n",
    "def calculate_raw_rsv(p_list):\n",
    "    #RSV = (收盤價-9日低值)/(9日高值-9日低值)\n",
    "    p_min, p_max, p_last = get_min_max_last(p_list)\n",
    "    rsv = (p_last - p_min)/(p_max - p_min)\n",
    "    return rsv\n",
    "def calculate_rsv(p_9_list, k_prev, d_prev):\n",
    "    #計算加權後的RSV，p_9_list=>9日收盤價\n",
    "    rrsv = calculate_raw_rsv(p_9_list)\n",
    "    k_curr = (1/3)*rrsv + (2/3)*k_prev\n",
    "    d_curr = (1/3)*k_curr + (2/3)*d_prev\n",
    "    return [k_curr, d_curr]\n",
    "#計算隨機指標（Stochastic Oscillator，KD），原名%K&%D\n",
    "def calculate_KD(close_p_list):\n",
    "    win_len = 9 #看過去 9 日值\n",
    "    #RSV = (收盤價-9日低值)/(9日高值-9日低值)\n",
    "    #K_curr = 1/3*RSV + 2/3*K_prev\n",
    "    #D_curr = 1/3*K_curr + 2/3*D_prev\n",
    "    if len(close_p_list) < win_len:\n",
    "        return None\n",
    "    elif len(close_p_list) == win_len:\n",
    "        #無前日K, D時，以0.5帶入\n",
    "        return calculate_rsv(close_p_list, 0.5, 0.5)\n",
    "    else:\n",
    "        kds = calculate_rsv(close_p_list[0:9], 0.5, 0.5)\n",
    "        for idx in range(1, (len(close_p_list)+1-9)):\n",
    "            p_9_list = close_p_list[idx: idx+9]\n",
    "            kds = calculate_rsv(p_9_list, kds[0], kds[1])\n",
    "        return kds\n",
    "calculate_KD=udf(calculate_KD, ArrayType(DoubleType()))\n",
    "\n",
    "#計算差離值DIF = 12日EMA - 26日EMA\n",
    "def calculate_DIF(close_p_list):\n",
    "    if len(close_p_list) < 26:\n",
    "        return None\n",
    "    else:\n",
    "        ema12 = calculate_ema_native(close_p_list, 12)\n",
    "        ema26 = calculate_ema_native(close_p_list, 26)\n",
    "        return ema12 - ema26\n",
    "calculate_DIF=udf(calculate_DIF, DoubleType())\n",
    "\n",
    "#計算MACD=(前一日MACD × (9 - 1) + 今日DIF × 2) ÷ (9 + 1)\n",
    "def calculate_MACD(dif_list, dif_curr):\n",
    "    win_len = 9\n",
    "    if len(dif_list) < win_len:\n",
    "        return None\n",
    "    elif len(dif_list) == win_len:\n",
    "        #if len of list = win_len then return avg, \n",
    "        return avg_list(dif_list)\n",
    "    else:\n",
    "        #MACD=(前一日MACD × (9 - 1) + 今日DIF × 2) ÷ (9 + 1)\n",
    "        macd = avg_list(dif_list[:win_len])\n",
    "        for price in dif_list[win_len:]:\n",
    "            macd = (macd*(win_len-1)+dif_curr*2)/(win_len+1)\n",
    "        return macd\n",
    "calculate_MACD=udf(calculate_MACD, DoubleType())\n",
    "\n",
    "#計算相對強弱指數(RSI)\n",
    "def calculate_RSI(close_p_list):\n",
    "    win_len = 9\n",
    "    if len(close_p_list) < (win_len + 1):\n",
    "        return None\n",
    "    else:\n",
    "        cur_list = close_p_list[1:]\n",
    "        prv_list = close_p_list[0:-1]\n",
    "        p_dif_list = list(map(lambda x,y : x - y, cur_list, prv_list)) #dif list\n",
    "        u_list = []\n",
    "        d_list = []\n",
    "        for dif in p_dif_list:\n",
    "            if dif == 0:\n",
    "                #若兩天價格相同，則U及D皆等於零\n",
    "                u_list.append(0)\n",
    "                d_list.append(0)\n",
    "            elif dif > 0:\n",
    "                #在價格上升的日子, U = diff, D = 0\n",
    "                u_list.append(dif)\n",
    "                d_list.append(0)\n",
    "            else:\n",
    "                #在價格下跌的日子, U = 0, D = abs(diff)\n",
    "                u_list.append(0)\n",
    "                d_list.append(abs(dif))\n",
    "        #RSI = ema(u,9)/(ema(u,9)+ema(d,9))\n",
    "        ema_u = calculate_ema_native(u_list, win_len)\n",
    "        ema_d = calculate_ema_native(d_list, win_len)\n",
    "        return ema_u/(ema_u + ema_d)\n",
    "calculate_RSI=udf(calculate_RSI, DoubleType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- etf_id: string (nullable = true)\n",
      " |-- etf_date: string (nullable = true)\n",
      " |-- etf_name: string (nullable = true)\n",
      " |-- etf_open: double (nullable = true)\n",
      " |-- etf_high: double (nullable = true)\n",
      " |-- etf_low: double (nullable = true)\n",
      " |-- etf_close: double (nullable = true)\n",
      " |-- etf_count: double (nullable = true)\n",
      " |-- row_idx: integer (nullable = true)\n",
      " |-- close_price_raw: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- EMA5: double (nullable = true)\n",
      " |-- EMA10: double (nullable = true)\n",
      " |-- EMA20: double (nullable = true)\n",
      " |-- BIAS: double (nullable = true)\n",
      " |-- KD: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- K: double (nullable = true)\n",
      " |-- D: double (nullable = true)\n",
      " |-- DIF: double (nullable = true)\n",
      " |-- dif_list: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- MACD: double (nullable = true)\n",
      " |-- RSI: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#calculate ema [5,10,20] #cannot remove row_idx, row_idx for next window usage\n",
    "tetfp_dt2=tetfp_dt.withColumn(\"row_idx\", rank().over(wsSpec_etf)) \\\n",
    "    .withColumn(\"close_price_raw\", collect_list(col('etf_close')).over(wsSpec_etf_close_price_raw)) \\\n",
    "    .withColumn(\"EMA5\", calculate_ema(col(\"close_price_raw\"), lit(5))) \\\n",
    "    .withColumn(\"EMA10\", calculate_ema(col(\"close_price_raw\"), lit(10))) \\\n",
    "    .withColumn(\"EMA20\", calculate_ema(col(\"close_price_raw\"), lit(20))) \\\n",
    "    .withColumn(\"BIAS\", calculate_bias(col(\"close_price_raw\"))) \\\n",
    "    .withColumn(\"KD\", calculate_KD(col(\"close_price_raw\"))) \\\n",
    "    .withColumn(\"K\", col(\"KD\")[0]).withColumn(\"D\", col(\"KD\")[1]) \\\n",
    "    .withColumn(\"DIF\", calculate_DIF(col(\"close_price_raw\"))) \\\n",
    "    .withColumn(\"dif_list\", collect_list(col('DIF')).over(wsSpec_etf_dif_raw)) \\\n",
    "    .withColumn(\"MACD\", calculate_MACD(col(\"dif_list\"), col(\"DIF\"))) \\\n",
    "    .withColumn(\"RSI\", calculate_RSI(col(\"close_price_raw\")))\n",
    "    \n",
    "tetfp_dt2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1415.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 25 in stage 107.0 failed 1 times, most recent failure: Lost task 25.0 in stage 107.0 (TID 2737, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\dev\\spark-2.2.1\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 177, in main\n  File \"C:\\dev\\spark-2.2.1\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in process\n  File \"C:\\dev\\spark-2.2.1\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 220, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"C:\\dev\\spark-2.2.1\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"C:\\dev\\spark-2.2.1\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 209, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"C:\\dev\\spark-2.2.1\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 71, in <lambda>\n  File \"<ipython-input-32-761cdf90d11e>\", line 108, in calculate_RSI\nValueError: not enough values to unpack (expected 2, got 0)\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:336)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2861)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2150)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2150)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2842)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2841)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2150)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2363)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:241)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\dev\\spark-2.2.1\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 177, in main\n  File \"C:\\dev\\spark-2.2.1\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in process\n  File \"C:\\dev\\spark-2.2.1\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 220, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"C:\\dev\\spark-2.2.1\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"C:\\dev\\spark-2.2.1\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 209, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"C:\\dev\\spark-2.2.1\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 71, in <lambda>\n  File \"<ipython-input-32-761cdf90d11e>\", line 108, in calculate_RSI\nValueError: not enough values to unpack (expected 2, got 0)\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-5d2a3ee47cf9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtetfp_dt2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"etf_id='0050   '\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m\"RSI\"\u001b[0m\u001b[1;33m)\u001b[0m             \u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m45\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#          .toPandas().to_csv(Path.replace(\"file:\",\"\") + \"/taetfp_BIAS.csv\",index=False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\spark-2.2.1\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate)\u001b[0m\n\u001b[0;32m    334\u001b[0m         \"\"\"\n\u001b[0;32m    335\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\spark-2.2.1\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\spark-2.2.1\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\spark-2.2.1\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1415.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 25 in stage 107.0 failed 1 times, most recent failure: Lost task 25.0 in stage 107.0 (TID 2737, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\dev\\spark-2.2.1\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 177, in main\n  File \"C:\\dev\\spark-2.2.1\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in process\n  File \"C:\\dev\\spark-2.2.1\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 220, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"C:\\dev\\spark-2.2.1\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"C:\\dev\\spark-2.2.1\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 209, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"C:\\dev\\spark-2.2.1\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 71, in <lambda>\n  File \"<ipython-input-32-761cdf90d11e>\", line 108, in calculate_RSI\nValueError: not enough values to unpack (expected 2, got 0)\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:336)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2861)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2150)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2150)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2842)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2841)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2150)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2363)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:241)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\dev\\spark-2.2.1\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 177, in main\n  File \"C:\\dev\\spark-2.2.1\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in process\n  File \"C:\\dev\\spark-2.2.1\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 220, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"C:\\dev\\spark-2.2.1\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"C:\\dev\\spark-2.2.1\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 209, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"C:\\dev\\spark-2.2.1\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 71, in <lambda>\n  File \"<ipython-input-32-761cdf90d11e>\", line 108, in calculate_RSI\nValueError: not enough values to unpack (expected 2, got 0)\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:144)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:87)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "tetfp_dt2.filter(\"etf_id='0050   '\").select(\"row_idx\" , \"BIAS\", \"K\", \"D\", \"DIF\", \"MACD\", \"RSI\") \\\n",
    "            .show(45)\n",
    "#          .toPandas().to_csv(Path.replace(\"file:\",\"\") + \"/taetfp_BIAS.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#匯出成pandas\n",
    "etf_pd_50 = tetfp_dt2.filter(\"etf_id='0050   '\").select(\"row_idx\", \"etf_date\" ,\"etf_close\", \"EMA5\", \"EMA10\", \"EMA20\", \"BIAS\") \\\n",
    "                .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#圖形化顯示\n",
    "#aetf_pd_50.set_index('etf_date') #set_index 後配合 loc select 出區段資料來看\n",
    "etf_pd_50_part = etf_pd_50.loc[1200:]\n",
    "etf_pd_50_part.etf_close.plot(x='row_idx', y='etf_close', style='b--', label=\"etf_close\")\n",
    "etf_pd_50_part.EMA5.plot(x='row_idx', y='EMA5', label=\"EMA5\", style='r-')\n",
    "etf_pd_50_part.EMA10.plot(x='row_idx', y='EMA10', label=\"EMA10\", style='g-')\n",
    "etf_pd_50_part.EMA20.plot(x='row_idx', y='EMA20', label=\"EMA20\", style='y-')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr between  EMA5  and etf_close:  0.9903396716685617\n",
      "corr between  EMA10  and etf_close:  0.9811876234371906\n",
      "corr between  EMA20  and etf_close:  0.9640948902379294\n",
      "corr between  BIAS  and etf_close:  0.10899567830506815\n",
      "corr between  K  and etf_close:  0.038137367336553714\n",
      "corr between  D  and etf_close:  0.044052741412852245\n",
      "corr between BIAS and etf_close_diff:  -0.04429838763677454\n"
     ]
    }
   ],
   "source": [
    "#計算各欄位與收盤價之相關性\n",
    "corr_cols = ['EMA5','EMA10','EMA20','BIAS','K','D']\n",
    "for col in corr_cols:\n",
    "    print('corr between ', col , ' and etf_close: ', str(tetfp_dt2.corr(col, 'etf_close')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47.31, 47.0, 46.79, 46.49, 46.66]\n",
      "[46.92, 47.31, 47.0, 46.79, 46.49]\n",
      "[0.39000000000000057, -0.3100000000000023, -0.21000000000000085, -0.29999999999999716, 0.1699999999999946]\n",
      "46.66\n",
      "46.902\n",
      "1   2\n",
      "3   4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ll = [46.92, 47.31, 47.0, 46.79, 46.49, 46.66, 47.0, 46.96, 47.0]\n",
    "ll = [46.92, 47.31, 47.0, 46.79, 46.49, 46.66]\n",
    "win_len=5\n",
    "print(ll[1:])\n",
    "print(ll[0: -1])\n",
    "print(list(map(lambda x,y : x - y, ll[1:], ll[0: -1])))\n",
    "for x in ll[win_len:]:\n",
    "    print(x)\n",
    "ema = sum(ll[:win_len])/len(ll[:win_len])\n",
    "print(ema)\n",
    "for price in ll[win_len:]:\n",
    "    ema = (ema*(win_len-1)+price*2)/(win_len+1)\n",
    "tup1, tup2 = (1,2)\n",
    "print(tup1, ' ', tup2)\n",
    "tup = (3,4)\n",
    "print(tup[0], ' ', tup[1])\n",
    "list(range(0,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

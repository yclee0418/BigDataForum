{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Url: https://tbrain.trendmicro.com.tw/Competitions/Details/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, udf, lag, rank, lit,trim\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "global Path\n",
    "global DoEval #是否進行模型評估\n",
    "\n",
    "DoEval = True\n",
    "next_date_range = [\"20180514\", \"20180515\", \"20180516\", \"20180517\", \"20180518\"] #設定預測區間\n",
    "ignore_dates = [\"\"]#設定排除日(如端午節)\n",
    "#next_date_range = [\"20180507\", \"20180508\", \"20180509\", \"20180510\", \"20180511\"] #設定預測區間\n",
    "#ignore_dates = [\"\"]#設定排除日(如端午節)\n",
    "#next_date_range = [\"20180430\", \"20180501\", \"20180502\", \"20180503\", \"20180504\"] #設定預測區間\n",
    "#ignore_dates = [\"20180501\"]#設定排除日(如端午節)\n",
    "predict_start_date = next_date_range[0]\n",
    "\n",
    "if sc.master[0:5]==\"local\":\n",
    "    #Path = \"file:/c:/D Drive/work/bigData/pySpark/TBrain_Round2_DataSet_20180511\"\n",
    "    #Path = \"file:/Users/yungchuanlee/Documents/learn/AI競賽/ETF預測/TBrain_Round2_DataSet_20180518\"\n",
    "    Path = \"file:/home/hduser/app/bigdata/competition/etf/TBrain_Round2_DataSet_20180518\"\n",
    "else:\n",
    "    Path = \"hdfs://master:9000/user/hduser\"\n",
    "Path = \"file:/home/hduser/app/bigdata/competition/etf/TBrain_Round2_DataSet_20180518\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spark://sparklab:7077'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define alias of columns\n",
    "col_alias_etf= {\"代碼\":\"etf_id\", \"日期\": \"etf_date\", \"中文簡稱\": \"etf_name\", \"開盤價(元)\":\"etf_open\", \n",
    "            \"最高價(元)\":\"etf_high\", \"最低價(元)\":\"etf_low\", \"收盤價(元)\":\"etf_close\", \"成交張數(張)\":\"etf_count\"}\n",
    "col_alias_stock= {\"代碼\":\"stock_id\", \"日期\": \"stock_date\", \"中文簡稱\": \"stock_name\", \"開盤價(元)\":\"stock_open\", \n",
    "            \"最高價(元)\":\"stock_high\", \"最低價(元)\":\"stock_low\", \"收盤價(元)\":\"stock_close\", \"成交張數(張)\":\"stock_count\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#udf\n",
    "def to_double(str_val):\n",
    "    return float(str_val.replace(\",\",\"\"))\n",
    "to_double=udf(to_double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def function to read data (因檔案格式都相同)\n",
    "def read_data(file_name, col_alias):\n",
    "    str_cols = [\"代碼\",\"日期\", \"中文簡稱\"]\n",
    "    raw_data = spark.read.option(\"encoding\", \"Big5\").csv(Path + \"/\" + file_name, header=True, sep=\",\")\n",
    "    print(\"Total \" + file_name + \" count: \" + str(raw_data.count()))\n",
    "    #rename cols and correct type \n",
    "    num_cols = [col_name for col_name in raw_data.columns if col_name not in str_cols]\n",
    "    final_data=raw_data.select( [col(str_col_name).alias(col_alias[str_col_name]) for str_col_name in str_cols] + \n",
    "                                  [to_double(col(num_col_name)).cast(\"double\").alias(col_alias[num_col_name]) for num_col_name in num_cols] )\n",
    "    final_data.printSchema()\n",
    "    final_data.show(5)\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting import tetfp.csv(台灣18檔ETF股價資料)...\n",
      "Total tetfp.csv count: 19306\n",
      "root\n",
      " |-- etf_id: string (nullable = true)\n",
      " |-- etf_date: string (nullable = true)\n",
      " |-- etf_name: string (nullable = true)\n",
      " |-- etf_open: double (nullable = true)\n",
      " |-- etf_high: double (nullable = true)\n",
      " |-- etf_low: double (nullable = true)\n",
      " |-- etf_close: double (nullable = true)\n",
      " |-- etf_count: double (nullable = true)\n",
      "\n",
      "+-------+--------+----------------+--------+--------+-------+---------+---------+\n",
      "| etf_id|etf_date|        etf_name|etf_open|etf_high|etf_low|etf_close|etf_count|\n",
      "+-------+--------+----------------+--------+--------+-------+---------+---------+\n",
      "|0050   |20130102|元大台灣50          |    54.0|   54.65|   53.9|     54.4|  16487.0|\n",
      "|0050   |20130103|元大台灣50          |    54.9|   55.05|  54.65|    54.85|  29020.0|\n",
      "|0050   |20130104|元大台灣50          |   54.85|   54.85|   54.4|     54.5|   9837.0|\n",
      "|0050   |20130107|元大台灣50          |   54.55|   54.55|   53.9|    54.25|   8910.0|\n",
      "|0050   |20130108|元大台灣50          |    54.0|    54.2|  53.65|     53.9|  12507.0|\n",
      "+-------+--------+----------------+--------+--------+-------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"starting import tetfp.csv(台灣18檔ETF股價資料)...\")\n",
    "tetfp_dt=read_data(\"tetfp.csv\", col_alias_etf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|            etf_date|\n",
      "+-------+--------------------+\n",
      "|  count|               19306|\n",
      "|   mean|2.0153649878068995E7|\n",
      "| stddev|    15917.8419890693|\n",
      "|    min|            20130102|\n",
      "|    max|            20180518|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#EDA\n",
    "#range of date\n",
    "tetfp_dt.describe('etf_date').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"starting import taetfp.csv(台灣18檔ETF調整後股價資料)...\")\n",
    "# taetfp_dt=read_data(\"taetfp.csv\", col_alias_etf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(\"starting import tsharep.csv(台灣個股股價資料)...\")\n",
    "# tsharep_dt=read_data(\"tsharep.csv\", col_alias_stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"starting import tasharep.csv(台灣個股調整後股價資料)...\")\n",
    "# tasharep_dt=read_data(\"tasharep.csv\", col_alias_stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql.functions import lag, col, avg,collect_list, lit\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import ArrayType, DoubleType, IntegerType\n",
    "#declare previous row windows\n",
    "wsSpec_etf = Window.partitionBy('etf_id').orderBy('etf_date') #time window for normal case\n",
    "wsSpec_etf_close_price_raw = Window.partitionBy('etf_id').orderBy('row_idx').rangeBetween(-sys.maxsize, -1)\n",
    "wsSpec_etf_dif_raw = Window.partitionBy('etf_id').orderBy('row_idx').rangeBetween(-sys.maxsize, 0)\n",
    "def avg_list(p_list):\n",
    "    #計算數字list的平均值\n",
    "    return sum(p_list)/len(p_list)\n",
    "#計算EMA的udf\n",
    "def calculate_ema_native(close_p_list, window_len):\n",
    "    #透過歷史收盤價計算\n",
    "    if len(close_p_list) < window_len:\n",
    "        return None\n",
    "    elif len(close_p_list) == window_len:\n",
    "        #if len of list = win_len then return avg, \n",
    "        return avg_list(close_p_list)\n",
    "    else:\n",
    "        #else EMA[t] =(EMA[t-1]*(win_len-1)+close[t]*2)/(win_len+1)\n",
    "        ema = avg_list(close_p_list[:window_len])\n",
    "        for price in close_p_list[window_len:]:\n",
    "            ema = (ema*(window_len-1)+price*2)/(window_len+1)\n",
    "        return ema\n",
    "calculate_ema=udf(calculate_ema_native, DoubleType())\n",
    "#計算BIAS的udf\n",
    "def calculate_bias(close_p_list):\n",
    "    #計算前日收盤價與N日均線之差比: (close price - MA)/MA   ,Paper 建議用20日MA\n",
    "    #因要預測今日的收盤價，故計算前日收盤價與前20日均線\n",
    "    if len(close_p_list) < 21:\n",
    "        return None\n",
    "    else:\n",
    "        list_len = len(close_p_list)\n",
    "        p_close = close_p_list[-1]\n",
    "        cal_list = close_p_list[list_len-21: list_len-1]\n",
    "        return p_close - avg_list(cal_list)\n",
    "calculate_bias=udf(calculate_bias, DoubleType())\n",
    "\n",
    "def get_min_max_last(p_list):\n",
    "    #找出list中最大最小和最後一個值, 回傳(min, max, last)\n",
    "    return (min(p_list), max(p_list), p_list[-1])\n",
    "def calculate_raw_rsv(p_list):\n",
    "    #RSV = (收盤價-9日低值)/(9日高值-9日低值)\n",
    "    p_min, p_max, p_last = get_min_max_last(p_list)\n",
    "    rsv = (p_last - p_min)/(p_max - p_min)\n",
    "    return rsv\n",
    "def calculate_rsv(p_9_list, k_prev, d_prev):\n",
    "    #計算加權後的RSV，p_9_list=>9日收盤價\n",
    "    rrsv = calculate_raw_rsv(p_9_list)\n",
    "    k_curr = (1/3)*rrsv + (2/3)*k_prev\n",
    "    d_curr = (1/3)*k_curr + (2/3)*d_prev\n",
    "    return [k_curr, d_curr]\n",
    "#計算隨機指標（Stochastic Oscillator，KD），原名%K&%D\n",
    "def calculate_KD(close_p_list):\n",
    "    win_len = 9 #看過去 9 日值\n",
    "    #RSV = (收盤價-9日低值)/(9日高值-9日低值)\n",
    "    #K_curr = 1/3*RSV + 2/3*K_prev\n",
    "    #D_curr = 1/3*K_curr + 2/3*D_prev\n",
    "    if len(close_p_list) < win_len:\n",
    "        return None\n",
    "    elif len(close_p_list) == win_len:\n",
    "        #無前日K, D時，以0.5帶入\n",
    "        return calculate_rsv(close_p_list, 0.5, 0.5)\n",
    "    else:\n",
    "        kds = calculate_rsv(close_p_list[0:9], 0.5, 0.5)\n",
    "        for idx in range(1, (len(close_p_list)+1-9)):\n",
    "            p_9_list = close_p_list[idx: idx+9]\n",
    "            kds = calculate_rsv(p_9_list, kds[0], kds[1])\n",
    "        return kds\n",
    "calculate_KD=udf(calculate_KD, ArrayType(DoubleType()))\n",
    "\n",
    "#計算差離值DIF = 12日EMA - 26日EMA\n",
    "def calculate_DIF_native(close_p_list):\n",
    "    if len(close_p_list) < 26:\n",
    "        return None\n",
    "    else:\n",
    "        ema12 = calculate_ema_native(close_p_list, 12)\n",
    "        ema26 = calculate_ema_native(close_p_list, 26)\n",
    "        return ema12 - ema26\n",
    "calculate_DIF=udf(calculate_DIF_native, DoubleType())\n",
    "\n",
    "#計算MACD=(前一日MACD × (9 - 1) + 今日DIF × 2) ÷ (9 + 1)\n",
    "def calculate_MACD(dif_list, dif_curr):\n",
    "    win_len = 9\n",
    "    if len(dif_list) < win_len:\n",
    "        return None\n",
    "    elif len(dif_list) == win_len:\n",
    "        #if len of list = win_len then return avg, \n",
    "        return avg_list(dif_list)\n",
    "    else:\n",
    "        #MACD=(前一日MACD × (9 - 1) + 今日DIF × 2) ÷ (9 + 1)\n",
    "        macd = avg_list(dif_list[:win_len])\n",
    "        for price in dif_list[win_len:]:\n",
    "            macd = (macd*(win_len-1)+dif_curr*2)/(win_len+1)\n",
    "        return macd\n",
    "calculate_MACD=udf(calculate_MACD, DoubleType())\n",
    "\n",
    "#計算相對強弱指數(RSI)\n",
    "def calculate_RSI(close_p_list):\n",
    "    win_len = 9\n",
    "    if len(close_p_list) < (win_len + 1):\n",
    "        return None\n",
    "    else:\n",
    "        cur_list = close_p_list[1:]\n",
    "        prv_list = close_p_list[0:-1]\n",
    "        p_dif_list = list(map(lambda x,y : x - y, cur_list, prv_list)) #dif list\n",
    "        u_list = []\n",
    "        d_list = []\n",
    "        for dif in p_dif_list:\n",
    "            if dif == 0:\n",
    "                #若兩天價格相同，則U及D皆等於零\n",
    "                u_list.append(0)\n",
    "                d_list.append(0)\n",
    "            elif dif > 0:\n",
    "                #在價格上升的日子, U = diff, D = 0\n",
    "                u_list.append(dif)\n",
    "                d_list.append(0)\n",
    "            else:\n",
    "                #在價格下跌的日子, U = 0, D = abs(diff)\n",
    "                u_list.append(0)\n",
    "                d_list.append(abs(dif))\n",
    "        #RSI = ema(u,9)/(ema(u,9)+ema(d,9))\n",
    "        ema_u = calculate_ema_native(u_list, win_len)\n",
    "        ema_d = calculate_ema_native(d_list, win_len)\n",
    "        return ema_u/(ema_u + ema_d)\n",
    "calculate_RSI=udf(calculate_RSI, DoubleType())\n",
    "\n",
    "#計算威廉指標（Williams %R）\n",
    "def calculate_WR(close_p_list):\n",
    "    win_len = 9\n",
    "    if len(close_p_list) < win_len:\n",
    "        return None\n",
    "    else:\n",
    "        p_list = close_p_list[len(close_p_list) - win_len :]\n",
    "        return 1.0 - calculate_raw_rsv(p_list)\n",
    "calculate_WR=udf(calculate_WR, DoubleType())\n",
    "\n",
    "#calculate profit of n-days\n",
    "def calculate_days_profit_natvie(close_p_list, days):\n",
    "    if len(close_p_list) < days+1:\n",
    "        return 0.0\n",
    "    cal_list = close_p_list[len(close_p_list)-days-1 : ]\n",
    "    #formula: [(1+daily_profit)*]-1\n",
    "    res = 1.0\n",
    "    for idx in range(0, days):\n",
    "        res = res * (1 + (cal_list[idx+1]/cal_list[idx]-1.0))\n",
    "    res = res -1.0\n",
    "    return res\n",
    "calculate_days_profit=udf(calculate_days_profit_natvie, DoubleType())\n",
    "\n",
    "from datetime import datetime\n",
    "def get_weekday_native(date_str):\n",
    "    return datetime.strptime(date_str.strip(), \"%Y%m%d\").weekday()+0.0\n",
    "get_weekday = udf(get_weekday_native , DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- etf_id: string (nullable = true)\n",
      " |-- etf_date: string (nullable = true)\n",
      " |-- etf_name: string (nullable = true)\n",
      " |-- etf_open: double (nullable = true)\n",
      " |-- etf_high: double (nullable = true)\n",
      " |-- etf_low: double (nullable = true)\n",
      " |-- etf_close: double (nullable = true)\n",
      " |-- etf_count: double (nullable = true)\n",
      " |-- row_idx: integer (nullable = true)\n",
      " |-- close_price_raw: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- EMA5: double (nullable = true)\n",
      " |-- EMA10: double (nullable = true)\n",
      " |-- EMA20: double (nullable = true)\n",
      " |-- EMA30: double (nullable = true)\n",
      " |-- BIAS: double (nullable = true)\n",
      " |-- KD: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- K: double (nullable = true)\n",
      " |-- D: double (nullable = true)\n",
      " |-- DIF: double (nullable = true)\n",
      " |-- dif_list: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- MACD: double (nullable = true)\n",
      " |-- RSI: double (nullable = true)\n",
      " |-- P_1: double (nullable = true)\n",
      " |-- P_5: double (nullable = true)\n",
      " |-- weekday: double (nullable = true)\n",
      " |-- WR: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#calculate ema [5,10,20] #cannot remove row_idx, row_idx for next window usage\n",
    "tetfp_dt2=tetfp_dt.withColumn(\"row_idx\", rank().over(wsSpec_etf)) \\\n",
    "    .withColumn(\"close_price_raw\", collect_list(col('etf_close')).over(wsSpec_etf_close_price_raw)) \\\n",
    "    .withColumn(\"EMA5\", calculate_ema(col(\"close_price_raw\"), lit(5))) \\\n",
    "    .withColumn(\"EMA10\", calculate_ema(col(\"close_price_raw\"), lit(10))) \\\n",
    "    .withColumn(\"EMA20\", calculate_ema(col(\"close_price_raw\"), lit(20))) \\\n",
    "    .withColumn(\"EMA30\", calculate_ema(col(\"close_price_raw\"), lit(30))) \\\n",
    "    .withColumn(\"BIAS\", calculate_bias(col(\"close_price_raw\"))) \\\n",
    "    .withColumn(\"KD\", calculate_KD(col(\"close_price_raw\"))) \\\n",
    "    .withColumn(\"K\", col(\"KD\")[0]).withColumn(\"D\", col(\"KD\")[1]) \\\n",
    "    .withColumn(\"DIF\", calculate_DIF(col(\"close_price_raw\"))) \\\n",
    "    .withColumn(\"dif_list\", collect_list(col('DIF')).over(wsSpec_etf_dif_raw)) \\\n",
    "    .withColumn(\"MACD\", calculate_MACD(col(\"dif_list\"), col(\"DIF\"))) \\\n",
    "    .withColumn(\"RSI\", calculate_RSI(col(\"close_price_raw\")))\\\n",
    "    .withColumn(\"P_1\", calculate_days_profit(col(\"close_price_raw\"), lit(1))) \\\n",
    "    .withColumn(\"P_5\", calculate_days_profit(col(\"close_price_raw\"), lit(5))) \\\n",
    "    .withColumn(\"weekday\", get_weekday(col(\"etf_date\"))) \\\n",
    "    .withColumn(\"WR\", calculate_WR(col(\"close_price_raw\")))\n",
    "\n",
    "tetfp_dt2.cache()\n",
    "tetfp_dt2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tetfp_dt2.filter(\"etf_id='0050   '\").select(\"row_idx\", \"etf_close\", \"MACD\", \"RSI\", \"WR\", 'weekday') \\\n",
    "#             .show()\n",
    "#          .toPandas().to_csv(Path.replace(\"file:\",\"\") + \"/taetfp_BIAS.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#匯出成pandas\n",
    "# etf_pd_50 = tetfp_dt2.filter(\"etf_id='0050   '\").select(\"row_idx\", \"etf_date\" ,\"etf_close\", \"EMA5\", \"EMA10\", \"EMA20\", \"BIAS\") \\\n",
    "#                 .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#圖形化顯示\n",
    "#aetf_pd_50.set_index('etf_date') #set_index 後配合 loc select 出區段資料來看\n",
    "# etf_pd_50_part = etf_pd_50.loc[1200:]\n",
    "# etf_pd_50_part.etf_close.plot(x='row_idx', y='etf_close', style='b--', label=\"etf_close\")\n",
    "# etf_pd_50_part.EMA5.plot(x='row_idx', y='EMA5', label=\"EMA5\", style='r-')\n",
    "# etf_pd_50_part.EMA10.plot(x='row_idx', y='EMA10', label=\"EMA10\", style='g-')\n",
    "# etf_pd_50_part.EMA20.plot(x='row_idx', y='EMA20', label=\"EMA20\", style='y-')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#計算各欄位與收盤價之相關性\n",
    "# corr_cols = ['EMA5','EMA10','EMA20','BIAS','K','D']\n",
    "# for col in corr_cols:\n",
    "#     print('corr between ', col , ' and etf_close: ', str(tetfp_dt2.corr(col, 'etf_close')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler, StandardScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "tot_dt = tetfp_dt2.filter(\"MACD is not null and EMA30 is not null\") \\\n",
    "    .select(\"etf_id\", \"etf_date\", \"row_idx\", \"EMA5\", \"EMA10\", \"EMA20\", \"EMA30\", \"BIAS\", \"K\", \"D\", \"DIF\", \"MACD\", \"RSI\", \n",
    "            \"P_1\", \"P_5\", (col(\"WR\")*-1.0).alias(\"WR\"), \"etf_close\") \\\n",
    "    .orderBy(\"etf_id\", \"etf_date\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total etf_extraFeatures_0518_L37.csv count: 15816\n",
      "root\n",
      " |-- etf_id: string (nullable = true)\n",
      " |-- etf_date: string (nullable = true)\n",
      " |-- etf_close: string (nullable = true)\n",
      " |-- bias80: string (nullable = true)\n",
      " |-- d80: string (nullable = true)\n",
      " |-- dif80: string (nullable = true)\n",
      " |-- k80: string (nullable = true)\n",
      " |-- macd80: string (nullable = true)\n",
      " |-- p1_80: string (nullable = true)\n",
      " |-- p5_80: string (nullable = true)\n",
      " |-- rsi80: string (nullable = true)\n",
      " |-- wr80: string (nullable = true)\n",
      " |-- close80: string (nullable = true)\n",
      "\n",
      "+------+--------+---------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+------------------+-------------------+------------------+\n",
      "|etf_id|etf_date|etf_close|            bias80|                d80|              dif80|                k80|             macd80|              p1_80|             p5_80|             rsi80|               wr80|           close80|\n",
      "+------+--------+---------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+------------------+-------------------+------------------+\n",
      "|  0050|20140102|    58.55| 0.821453691187042|0.20302344276162565| 0.4701424208413601| 0.5380404418897358| 0.4701424208413602| 0.4405449405322566|0.7405251364204671|0.8027707571568459| -0.674945072612263|0.6151829138780527|\n",
      "|  0050|20140103|    57.85|0.8248879991388902| 0.2022321216159159| 0.4552523634602419| 0.5330694906420842|0.45525236346024195| 0.4379825696820671|0.7363702669362353|0.8003758576295509| -0.670675450898402|0.6290729437063292|\n",
      "|  0050|20140106|     57.7|0.8269672643209345|0.18387502545951734|  0.456671476050673| 0.5075046835177756|  0.456671476050673| 0.4538842379506356|0.7314765196459841|0.7894587980833705|-0.6523332811706686|0.6249778775668015|\n",
      "|  0050|20140107|     57.7|0.8215332557405478|0.19501364796837242|0.44675687532711056| 0.5138982603582064|0.44675687532711056|0.43087276200408153|0.7306744104261015|0.7793915259111087|-0.6523621241892807|0.6259754031542513|\n",
      "|  0050|20140108|     57.8|0.8257117808574338|0.19219117031483465| 0.4461943152906089| 0.5077698046811384| 0.4461943152906089| 0.4306766454433117| 0.738421322052804| 0.785985113825319|-0.6560508420585248|0.6549559242357561|\n",
      "|  0050|20140109|    57.55|0.8285626084157484|0.20011252546522876| 0.4428719755200677| 0.5126465409688764| 0.4428719755200677| 0.4204471944269351|0.7380028680956218|0.7869418179234883|-0.6625054527264053|0.6452448493133759|\n",
      "|  0050|20140110|    57.55| 0.833268253958017|0.21776665868375772| 0.4390503338819172| 0.5261818482777718| 0.4390503338819171| 0.4261834611161642| 0.737697859929427| 0.791919719857822|-0.6781793585831362|0.6248092604650378|\n",
      "|  0050|20140113|     57.9|0.8334574626836009|0.22309466447117218|0.43462330850917735| 0.5261982384537237|0.43462330850917735|0.43278970769840364|  0.73553088698968|0.7917706846608112|-0.6781793585831362| 0.622184700522603|\n",
      "|  0050|20140114|     57.8|0.8243136968364723|0.19564262523853354|  0.422148498396534| 0.5018788900648714|  0.422148498396534|0.41358993894128065|0.7301117801473798|0.7809383915106337|-0.6646941724366918|0.5756485811687085|\n",
      "|  0050|20140115|     58.4|0.8126466482054907|0.15418027584187174|0.39883281974681173|0.47598152082903594| 0.3988328197468118| 0.3936060120824073|0.7243567831815549|0.7726202298998859|-0.6536034552666188|0.5299340433040908|\n",
      "+------+--------+---------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+------------------+-------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chaos_file_name=\"etf_extraFeatures_0518_L37.csv\"\n",
    "#$$$\n",
    "chaos_raw_data = spark.read.csv(Path + \"/\" + chaos_file_name, header=True, sep=\",\") \\\n",
    "    .filter(\"etf_date > '20140101'\")\n",
    "print(\"Total \" + chaos_file_name + \" count: \" + str(chaos_raw_data.count()))\n",
    "chaos_raw_data.printSchema()\n",
    "chaos_raw_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- etf_id: string (nullable = true)\n",
      " |-- etf_date: string (nullable = true)\n",
      " |-- bias80: double (nullable = true)\n",
      " |-- d80: double (nullable = true)\n",
      " |-- dif80: double (nullable = true)\n",
      " |-- k80: double (nullable = true)\n",
      " |-- macd80: double (nullable = true)\n",
      " |-- p1_80: double (nullable = true)\n",
      " |-- p5_80: double (nullable = true)\n",
      " |-- rsi80: double (nullable = true)\n",
      " |-- wr80: double (nullable = true)\n",
      " |-- close80: double (nullable = true)\n",
      "\n",
      "+------+--------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+------------------+------------------+------------------+\n",
      "|etf_id|etf_date|            bias80|                d80|              dif80|                k80|             macd80|              p1_80|             p5_80|             rsi80|              wr80|           close80|\n",
      "+------+--------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+------------------+------------------+------------------+\n",
      "|  0050|20140102| 0.821453691187042|0.20302344276162565| 0.4701424208413601| 0.5380404418897358| 0.4701424208413602| 0.4405449405322566|0.7405251364204671|0.8027707571568459| 0.674945072612263|0.6151829138780527|\n",
      "|  0050|20140103|0.8248879991388902| 0.2022321216159159| 0.4552523634602419| 0.5330694906420842|0.45525236346024195| 0.4379825696820671|0.7363702669362353|0.8003758576295509| 0.670675450898402|0.6290729437063292|\n",
      "|  0050|20140106|0.8269672643209345|0.18387502545951734|  0.456671476050673| 0.5075046835177756|  0.456671476050673| 0.4538842379506356|0.7314765196459841|0.7894587980833705|0.6523332811706686|0.6249778775668015|\n",
      "|  0050|20140107|0.8215332557405478|0.19501364796837242|0.44675687532711056| 0.5138982603582064|0.44675687532711056|0.43087276200408153|0.7306744104261015|0.7793915259111087|0.6523621241892807|0.6259754031542513|\n",
      "|  0050|20140108|0.8257117808574338|0.19219117031483465| 0.4461943152906089| 0.5077698046811384| 0.4461943152906089| 0.4306766454433117| 0.738421322052804| 0.785985113825319|0.6560508420585248|0.6549559242357561|\n",
      "|  0050|20140109|0.8285626084157484|0.20011252546522876| 0.4428719755200677| 0.5126465409688764| 0.4428719755200677| 0.4204471944269351|0.7380028680956218|0.7869418179234883|0.6625054527264053|0.6452448493133759|\n",
      "|  0050|20140110| 0.833268253958017|0.21776665868375772| 0.4390503338819172| 0.5261818482777718| 0.4390503338819171| 0.4261834611161642| 0.737697859929427| 0.791919719857822|0.6781793585831362|0.6248092604650378|\n",
      "|  0050|20140113|0.8334574626836009|0.22309466447117218|0.43462330850917735| 0.5261982384537237|0.43462330850917735|0.43278970769840364|  0.73553088698968|0.7917706846608112|0.6781793585831362| 0.622184700522603|\n",
      "|  0050|20140114|0.8243136968364723|0.19564262523853354|  0.422148498396534| 0.5018788900648714|  0.422148498396534|0.41358993894128065|0.7301117801473798|0.7809383915106337|0.6646941724366918|0.5756485811687085|\n",
      "|  0050|20140115|0.8126466482054907|0.15418027584187174|0.39883281974681173|0.47598152082903594| 0.3988328197468118| 0.3936060120824073|0.7243567831815549|0.7726202298998859|0.6536034552666188|0.5299340433040908|\n",
      "+------+--------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+------------------+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chaos_dt = chaos_raw_data.filter(\"wr80 != ''\") \\\n",
    "    .select(\"etf_id\", \"etf_date\", col(\"bias80\").cast(\"Double\"), col(\"d80\").cast(\"Double\"), col(\"dif80\").cast(\"Double\"), \n",
    "           col(\"k80\").cast(\"Double\"), col(\"macd80\").cast(\"Double\"), col(\"p1_80\").cast(\"Double\"), \n",
    "            col(\"p5_80\").cast(\"Double\"), col(\"rsi80\").cast(\"Double\"), (col(\"wr80\").cast(\"Double\")*-1.0).alias(\"wr80\"),\n",
    "           col(\"close80\").cast(\"Double\"))\n",
    "chaos_dt.printSchema()\n",
    "chaos_dt.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- etf_id: string (nullable = true)\n",
      " |-- etf_date: string (nullable = true)\n",
      " |-- row_idx: integer (nullable = true)\n",
      " |-- EMA5: double (nullable = true)\n",
      " |-- EMA10: double (nullable = true)\n",
      " |-- EMA20: double (nullable = true)\n",
      " |-- EMA30: double (nullable = true)\n",
      " |-- BIAS: double (nullable = true)\n",
      " |-- K: double (nullable = true)\n",
      " |-- D: double (nullable = true)\n",
      " |-- DIF: double (nullable = true)\n",
      " |-- MACD: double (nullable = true)\n",
      " |-- RSI: double (nullable = true)\n",
      " |-- P_1: double (nullable = true)\n",
      " |-- P_5: double (nullable = true)\n",
      " |-- WR: double (nullable = true)\n",
      " |-- etf_close: double (nullable = true)\n",
      " |-- bias80: double (nullable = true)\n",
      " |-- d80: double (nullable = true)\n",
      " |-- dif80: double (nullable = true)\n",
      " |-- k80: double (nullable = true)\n",
      " |-- macd80: double (nullable = true)\n",
      " |-- p1_80: double (nullable = true)\n",
      " |-- p5_80: double (nullable = true)\n",
      " |-- rsi80: double (nullable = true)\n",
      " |-- wr80: double (nullable = true)\n",
      " |-- close80: double (nullable = true)\n",
      "\n",
      "+-------+--------+-------+------------------+------------------+------------------+------------------+--------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------------+--------------------+--------------------+---------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+------------------+------------------+------------------+\n",
      "| etf_id|etf_date|row_idx|              EMA5|             EMA10|             EMA20|             EMA30|                BIAS|                  K|                  D|                DIF|               MACD|                RSI|                 P_1|                 P_5|                  WR|etf_close|            bias80|                d80|              dif80|                k80|             macd80|              p1_80|             p5_80|             rsi80|              wr80|           close80|\n",
      "+-------+--------+-------+------------------+------------------+------------------+------------------+--------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------------+--------------------+--------------------+---------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+------------------+------------------+------------------+\n",
      "|0050   |20140102|    247| 58.37698981216323| 58.05988634181784| 57.72952244398518| 57.56450411992626|  1.1200000000000045| 0.9410667436622058| 0.8755755995663493|0.35412323731589623|0.35412323731589634| 0.8125474690250128|-0.00170068027210879|0.015570934256055713|-0.06896551724137556|    58.55| 0.821453691187042|0.20302344276162565| 0.4701424208413601| 0.5380404418897358| 0.4701424208413602| 0.4405449405322566|0.7405251364204671|0.8027707571568459| 0.674945072612263|0.6151829138780527|\n",
      "|0050   |20140103|    248| 58.43465987477549|58.148997916032776| 57.80766316360564|57.628084499285855|  0.9099999999999895| 0.8989827673797417|   0.88337798883748|0.37396889343647643|0.37396889343647655|  0.701126271525996|-0.00255536626916...|0.012100259291270454|  -0.185185185185186|    57.85|0.8248879991388902| 0.2022321216159159| 0.4552523634602419| 0.5330694906420842|0.45525236346024195| 0.4379825696820671|0.7363702669362353|0.8003758576295509| 0.670675450898402|0.6290729437063292|\n",
      "|0050   |20140106|    249| 58.23977324985032|58.094634658572275| 57.81169524326225| 57.64240162836419| 0.15499999999999403| 0.6159885115864958| 0.7942481630871518|0.32941527480232224|0.32941527480232236| 0.3895361656283798|-0.01195559350982...|                 0.0| -0.9499999999999957|     57.7|0.8269672643209345|0.18387502545951734|  0.456671476050673| 0.5075046835177756|  0.456671476050673| 0.4538842379506356|0.7314765196459841|0.7894587980833705|0.6523332811706686|0.6249778775668015|\n",
      "|0050   |20140107|    250| 58.05984883323355|58.022882902468226| 57.80105760104681|  57.6461176523407|-0.03249999999999176| 0.4106590077243305| 0.6663851112995447| 0.2787887613447637| 0.2787887613447638|0.34809871141644283|-0.00259291270527...|-0.01029159519725...|                -1.0|     57.7|0.8215332557405478|0.19501364796837242|0.44675687532711056| 0.5138982603582064|0.44675687532711056|0.43087276200408153|0.7306744104261015|0.7793915259111087|0.6523621241892807|0.6259754031542513|\n",
      "|0050   |20140108|    251|  57.9398992221557|57.964176920201275| 57.79143306761379|57.649593932834854|-0.06000000000000...|0.27377267181622034| 0.5355142981384365|0.23594703414758555|0.23594703414758564|0.34809871141644283|                 0.0|-0.01870748299319...|                -1.0|     57.8|0.8257117808574338|0.19219117031483465| 0.4461943152906089| 0.5077698046811384| 0.4461943152906089| 0.4306766454433117| 0.738421322052804| 0.785985113825319|0.6560508420585248|0.6549559242357561|\n",
      "|0050   |20140109|    252|57.893266148103805|57.934326571073775| 57.79224896593628|57.659297550071315|0.037499999999994316| 0.2128181448471756|0.42794891370801624|0.20766993125354105|0.20766993125354113| 0.4131291369714023|0.001733102253032...|-0.01533219761499...| -0.9090909090909138|    57.55|0.8285626084157484|0.20011252546522876| 0.4428719755200677| 0.5126465409688764| 0.4428719755200677| 0.4204471944269351|0.7380028680956218|0.7869418179234883|0.6625054527264053|0.6452448493133759|\n",
      "|0050   |20140110|    253|57.778844098735874|57.864449012696724|  57.7691776358471|   57.652246095228|-0.21250000000000568|0.14187876323145038|0.33259219688249425|0.16320587706301382|0.16320587706301387| 0.3149487624587173|-0.00432525951557...|-0.01707941929974366|                -1.0|    57.55| 0.833268253958017|0.21776665868375772| 0.4390503338819172| 0.5261818482777718| 0.4390503338819171| 0.4261834611161642| 0.737697859929427| 0.791919719857822|0.6781793585831362|0.6248092604650378|\n",
      "|0050   |20140113|    254|57.702562732490584|57.807276464933686| 57.74830357529023| 57.64564957295523|-0.19750000000000512|0.09458584215430024|0.25325674530642955|0.12650947133467128|0.12650947133467133| 0.3149487624587173|                 0.0|-0.00518582541054...|                -1.0|     57.9|0.8334574626836009|0.22309466447117218|0.43462330850917735| 0.5261982384537237|0.43462330850917735|0.43278970769840364|  0.73553088698968|0.7917706846608112|0.6781793585831362| 0.622184700522603|\n",
      "|0050   |20140114|    255| 57.76837515499372| 57.82413528949119|57.762750853834014|57.662059277925856| 0.14500000000000313|0.16450650346518558|0.22367333135934822| 0.1242372506491094|0.12423725064910944|  0.549267065302081|0.006081668114682914|0.003466204506065...| -0.6956521739130437|     57.8|0.8243136968364723|0.19564262523853354|  0.422148498396534| 0.5018788900648714|  0.422148498396534|0.41358993894128065|0.7301117801473798|0.7809383915106337|0.6646941724366918|0.5756485811687085|\n",
      "|0050   |20140115|    256| 57.77891676999582| 57.81974705503824|  57.7662983915641| 57.67095867934999| 0.01749999999999119|0.19300433564345704|0.21345033278738446|0.11306400366585478|0.11306400366585483|0.48947351706573716|-0.00172711571675...|0.001733102253032...|               -0.75|     58.4|0.8126466482054907|0.15418027584187174|0.39883281974681173|0.47598152082903594| 0.3988328197468118| 0.3936060120824073|0.7243567831815549|0.7726202298998859|0.6536034552666188|0.5299340433040908|\n",
      "+-------+--------+-------+------------------+------------------+------------------+------------------+--------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------------+--------------------+--------------------+---------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+------------------+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tot_dt2 = tot_dt.join(chaos_dt, \n",
    "                        [trim(tot_dt.etf_id) == chaos_dt.etf_id , trim(tot_dt.etf_date) == chaos_dt.etf_date], \"inner\") \\\n",
    "    .drop(chaos_dt.etf_id).drop(chaos_dt.etf_date)\n",
    "\n",
    "tot_dt2.printSchema()\n",
    "tot_dt2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "#$$$將Feature合併為Vector 並作標準化\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"EMA5\", \"EMA10\", \"EMA20\", \"EMA30\", \"BIAS\", \"K\", \"D\", \"DIF\", \"MACD\", \n",
    "                \"RSI\",\"P_1\", \"P_5\", \"bias80\", \"d80\", \"dif80\", \"k80\", \"macd80\", \n",
    "                \"p1_80\", \"p5_80\", \"rsi80\", \"WR\", \"wr80\"],\n",
    "    outputCol=\"features\")\n",
    "tot_dt_1 = assembler.transform(tot_dt2)\n",
    "#minmax_scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"stdFeatures\")\n",
    "#scaler_model = minmax_scaler.fit(tot_dt_1)\n",
    "#std_scaler = StandardScaler(inputCol=\"features\", outputCol=\"stdFeatures\")\n",
    "#scaler_model = std_scaler.fit(tot_dt_1)\n",
    "#tot_dt_scale = scaler_model.transform(tot_dt_1)\n",
    "tot_dt_scale=tot_dt_1.withColumn(\"stdFeatures\", col(\"features\")) #$$$ 測試不作標準化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train count:  15602 , test count:  90\n",
      "+-------+--------+-------+--------------------+---------+\n",
      "| etf_id|etf_date|row_idx|         stdFeatures|etf_close|\n",
      "+-------+--------+-------+--------------------+---------+\n",
      "|0050   |20140102|    247|[58.3769898121632...|    58.55|\n",
      "|0050   |20140103|    248|[58.4346598747754...|    57.85|\n",
      "|0050   |20140106|    249|[58.2397732498503...|     57.7|\n",
      "|0050   |20140107|    250|[58.0598488332335...|     57.7|\n",
      "|0050   |20140108|    251|[57.9398992221557...|     57.8|\n",
      "|0050   |20140109|    252|[57.8932661481038...|    57.55|\n",
      "|0050   |20140110|    253|[57.7788440987358...|    57.55|\n",
      "|0050   |20140113|    254|[57.7025627324905...|     57.9|\n",
      "|0050   |20140114|    255|[57.7683751549937...|     57.8|\n",
      "|0050   |20140115|    256|[57.7789167699958...|     58.4|\n",
      "+-------+--------+-------+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------+--------+-------+--------------------+---------+\n",
      "| etf_id|etf_date|row_idx|         stdFeatures|etf_close|\n",
      "+-------+--------+-------+--------------------+---------+\n",
      "|0050   |20180514|   1313|[80.6007303437532...|     82.5|\n",
      "|0050   |20180515|   1314|[81.2338202291688...|    81.65|\n",
      "|0050   |20180516|   1315|[81.3725468194458...|    81.75|\n",
      "|0050   |20180517|   1316|[81.4983645462972...|     81.2|\n",
      "|0050   |20180518|   1317|[81.3989096975315...|    80.95|\n",
      "|0051   |20180514|   1313|[32.4457386404168...|    33.25|\n",
      "|0051   |20180515|   1314|[32.7138257602779...|     33.1|\n",
      "|0051   |20180516|   1315|[32.8425505068519...|    33.03|\n",
      "|0051   |20180517|   1316|[32.9050336712346...|    33.14|\n",
      "|0051   |20180518|   1317|[32.9833557808230...|     33.1|\n",
      "+-------+--------+-------+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###### 取出4/16~4/27 (共兩週資料作為測試集)\n",
    "train_dt = tot_dt_scale.filter(\"etf_date < '\" + predict_start_date + \"' and MACD is not null and EMA30 is not null\") \\\n",
    "    .select(\"etf_id\", \"etf_date\", \"row_idx\", \"stdFeatures\", \"etf_close\") \\\n",
    "    .orderBy(\"etf_id\", \"etf_date\", ascending=True)\n",
    "test_dt = tot_dt_scale.filter(\"etf_date >= '\" + predict_start_date + \"'\") \\\n",
    "    .select(\"etf_id\", \"etf_date\", \"row_idx\", \"stdFeatures\", \"etf_close\") \\\n",
    "    .orderBy(\"etf_id\", \"etf_date\", ascending=True)\n",
    "print('train count: ', str(train_dt.count()), ', test count: ', str(test_dt.count()))\n",
    "train_dt.show(10)\n",
    "test_dt.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_dt.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[etf_id: string, etf_date: string, row_idx: int, stdFeatures: vector, etf_close: double]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dt.cache()\n",
    "test_dt.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0050   ',\n",
       " '0051   ',\n",
       " '0052   ',\n",
       " '0053   ',\n",
       " '0054   ',\n",
       " '0055   ',\n",
       " '0056   ',\n",
       " '0057   ',\n",
       " '0058   ',\n",
       " '0059   ',\n",
       " '006201 ',\n",
       " '006203 ',\n",
       " '006204 ',\n",
       " '006208 ',\n",
       " '00690  ',\n",
       " '00692  ',\n",
       " '00701  ',\n",
       " '00713  ']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#取出etf的distinct id\n",
    "etf_ids = []\n",
    "for row in train_dt.select(\"etf_id\").distinct().orderBy(\"etf_id\").collect():\n",
    "    etf_ids.append(row[\"etf_id\"])\n",
    "etf_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#訓練Model及評估(RandomForestRegressor - etf_id wise) \n",
    "#-- accuracy:0.48, (RMSE) on test data = 0.938023 --> using stdScaler\n",
    "#-- accuracy:0.48, (RMSE) on test data = 0.87064 --> no Scaler\n",
    "#-- accuracy:0.56, (RMSE) on test data = 0.92546 --> use minmaxScaler\n",
    "#-- accuracy:0.513812 , (RMSE) on test data = 0.597397 --> use minmaxScaler and no Scaler(full data)\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "#$$$default param of RF: maxDepth=5, maxBins=32, numTrees=20\n",
    "rf = RandomForestRegressor(featuresCol=\"stdFeatures\",labelCol=\"etf_close\", \n",
    "                           maxDepth=12, maxBins=64)\n",
    "#記下對應etf_id的所有model\n",
    "model_map = {}\n",
    "for etfid in etf_ids:\n",
    "    train_data = train_dt.filter(\"etf_id='\" + etfid + \"'\")\n",
    "    rf_model = rf.fit(train_data)\n",
    "    model_map.update({etfid: rf_model})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------+---------+--------------------+--------------------+\n",
      "| etf_id|etf_date|row_idx|etf_close|     close_price_raw|            dif_list|\n",
      "+-------+--------+-------+---------+--------------------+--------------------+\n",
      "|0050   |20180511| 1312.0|     81.6|[54.4, 54.85, 54....|[0.35882671868262...|\n",
      "|0051   |20180511| 1312.0|    32.79|[26.09, 26.12, 26...|[0.23810818793990...|\n",
      "|0052   |20180511| 1312.0|     53.0|[32.72, 32.12, 32...|[0.30196772810150...|\n",
      "|0053   |20180511| 1312.0|    35.48|[23.26, 23.11, 22...|[0.15597485321855...|\n",
      "|0054   |20180511| 1312.0|    23.76|[19.4, 19.37, 19....|[0.09327655672724...|\n",
      "|0055   |20180511| 1312.0|    17.36|[11.47, 11.49, 11...|[0.31374969126963...|\n",
      "|0056   |20180511| 1312.0|     25.6|[22.95, 23.06, 22...|[0.31828091002715...|\n",
      "|0057   |20180511| 1312.0|    50.25|[31.94, 31.93, 31...|[0.20109705259728...|\n",
      "|0058   |20180511| 1312.0|    46.84|[32.12, 31.96, 32...|[-0.1346882469799...|\n",
      "|0059   |20180511| 1312.0|    42.52|[24.65, 24.67, 24...|[0.65108836931329...|\n",
      "|006201 |20180511| 1312.0|    13.95|[10.24, 10.28, 10...|[0.10256677527479...|\n",
      "|006203 |20180511| 1312.0|    37.91|[26.11, 26.5, 26....|[0.23702653878804...|\n",
      "|006204 |20180511| 1312.0|    53.95|[38.82, 39.06, 38...|[0.30591619908080...|\n",
      "|006208 |20180511| 1312.0|    47.82|[31.02, 31.28, 31...|[0.22969191539465...|\n",
      "|00690  |20180511|  275.0|     22.0|[19.91, 19.95, 19...|[0.12719407487044...|\n",
      "|00692  |20180511|  244.0|    21.21|[19.82, 19.81, 19...|[0.21096625135049...|\n",
      "|00701  |20180511|  179.0|    21.11|[20.09, 20.1, 20....|[-0.0364782011969...|\n",
      "|00713  |20180511|  150.0|    30.74|[29.88, 29.84, 29...|[0.10443575803092...|\n",
      "+-------+--------+-------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#find last records of all etf to be the base of next record\n",
    "tetf_dt_prod = tetfp_dt2.filter(\"etf_date < '\" + predict_start_date + \"' and MACD is not null\")\n",
    "tetf_max_idx = tetf_dt_prod.groupBy(\"etf_id\").max(\"row_idx\")\n",
    "tetf_max = tetf_max_idx.select(col(\"etf_id\"), col(\"max(row_idx)\").cast(\"Double\").alias(\"row_idx\")) \\\n",
    "    .join(tetf_dt_prod, [\"etf_id\", \"row_idx\"], \"inner\") \\\n",
    "    .select(\"etf_id\", \"etf_date\", \"row_idx\", \"etf_close\", \"close_price_raw\", \"dif_list\") \\\n",
    "    .orderBy('etf_id')\n",
    "\n",
    "tetf_max.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立產生feature之方法\n",
    "def create_feature(etf_id, date, orig_dt, chaos_dt):\n",
    "    new_dt = orig_dt.withColumn(\"EMA5\", calculate_ema(col(\"close_price_raw\"), lit(5))) \\\n",
    "    .withColumn(\"EMA10\", calculate_ema(col(\"close_price_raw\"), lit(10))) \\\n",
    "    .withColumn(\"EMA20\", calculate_ema(col(\"close_price_raw\"), lit(20))) \\\n",
    "    .withColumn(\"EMA30\", calculate_ema(col(\"close_price_raw\"), lit(30))) \\\n",
    "    .withColumn(\"BIAS\", calculate_bias(col(\"close_price_raw\"))) \\\n",
    "    .withColumn(\"KD\", calculate_KD(col(\"close_price_raw\"))) \\\n",
    "    .withColumn(\"K\", col(\"KD\")[0]).withColumn(\"D\", col(\"KD\")[1]) \\\n",
    "    .withColumn(\"MACD\", calculate_MACD(col(\"dif_list\"), col(\"DIF\"))) \\\n",
    "    .withColumn(\"RSI\", calculate_RSI(col(\"close_price_raw\")))\\\n",
    "    .withColumn(\"P_1\", calculate_days_profit(col(\"close_price_raw\"), lit(1))) \\\n",
    "    .withColumn(\"P_5\", calculate_days_profit(col(\"close_price_raw\"), lit(5))) \\\n",
    "    .withColumn(\"weekday\", get_weekday(col(\"etf_date\"))) \\\n",
    "    .withColumn(\"WR\", calculate_WR(col(\"close_price_raw\")))\n",
    "    filter_str = \"etf_id='\"+etf_id+\"' and etf_date='\"+date+\"'\"\n",
    "    #print('filter_str: ', filter_str)\n",
    "    chaos_pd = chaos_dt.filter(filter_str)\n",
    "    new_dt2 = new_dt.join(chaos_pd, \n",
    "                        [trim(new_dt.etf_id) == chaos_pd.etf_id , trim(new_dt.etf_date) == chaos_pd.etf_date], \"inner\") \\\n",
    "    .drop(chaos_pd.etf_id).drop(chaos_pd.etf_date)\n",
    "    return new_dt2\n",
    "#計算上或下的值(udf)\n",
    "def judge_up_down_pred(curr_price, close_price_list):\n",
    "    if len(close_price_list) == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        prev_price = close_price_list[-1]\n",
    "        if curr_price == prev_price:\n",
    "            return 0.0\n",
    "        elif curr_price > prev_price:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 2.0\n",
    "judge_up_down_pred=udf(judge_up_down_pred, DoubleType())\n",
    "#建立預測方法\n",
    "def doPredict(test_dt, etf_ids, model_dic):\n",
    "    predict_res = None\n",
    "    for etfid in etf_ids:\n",
    "        test_data = test_dt.filter(\"etf_id='\" + etfid + \"'\")\n",
    "        predicts = model_dic[etfid].transform(test_data)\n",
    "        if predict_res is None:\n",
    "            predict_res = predicts\n",
    "        else:\n",
    "            predict_res = predict_res.unionAll(predicts)\n",
    "    predict_res2 = predict_res.withColumn(\"pred_ud\", judge_up_down_pred(col(\"prediction\"), col(\"close_price_raw\")))\n",
    "    return predict_res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+-------+\n",
      "|etf_id|etf_date|prediction|pred_ud|\n",
      "+------+--------+----------+-------+\n",
      "+------+--------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = tetfp_dt2.select(\"etf_id\", \"etf_date\", col(\"row_idx\").cast(\"Double\"), \"close_price_raw\", \"DIF\", \"dif_list\").schema\n",
    "idx_plus = 0.0\n",
    "predict_res_final = None\n",
    "predict_range = [d for d in next_date_range if d not in ignore_dates]\n",
    "for date in predict_range:\n",
    "    next_rows = []\n",
    "    idx_plus = idx_plus +1\n",
    "    for row in tetf_max.collect():\n",
    "        close_price_raw = row[\"close_price_raw\"]\n",
    "        close_price_raw.append(row[\"etf_close\"])\n",
    "        dif_list = row[\"dif_list\"]\n",
    "        dif = calculate_DIF_native(close_price_raw)\n",
    "        dif_list.append(dif)\n",
    "        next_row = (row[\"etf_id\"], date, row[\"row_idx\"]+idx_plus, close_price_raw, dif ,dif_list)\n",
    "        next_rows.append(next_row)\n",
    "    orig_dt = spark.createDataFrame(next_rows, schema)\n",
    "    new_dt = create_feature(row[\"etf_id\"], date, orig_dt, chaos_dt) #加入預測用的feature\n",
    "    feature_dt_tmp = assembler.transform(new_dt)#作出 FeatureVector\n",
    "    #feature_dt = scaler_model.transform(feature_dt_tmp)\n",
    "    feature_dt = feature_dt_tmp.withColumn(\"stdFeatures\", col(\"features\")) #$$$  no scaler\n",
    "    #feature_dt.select(\"etf_id\", \"etf_date\", \"stdFeatures\").show(10, False)\n",
    "    #do predict using existed model\n",
    "    pred_res = doPredict(feature_dt, etf_ids, model_map)\n",
    "    #取出所有預測結果作合併，以進行後續成績計算\n",
    "    merged_dt = pred_res.select(\"etf_id\", \"etf_date\", \"prediction\", \"pred_ud\")\n",
    "    if predict_res_final is None:\n",
    "        predict_res_final = merged_dt\n",
    "    else:\n",
    "        predict_res_final = predict_res_final.unionAll(merged_dt)\n",
    "    tetf_max = pred_res.select(\"etf_id\", \"etf_date\", \"row_idx\", col(\"prediction\").alias(\"etf_close\"), \"close_price_raw\", \"dif_list\")\n",
    "    \n",
    "predict_res_final.cache()\n",
    "predict_res_final.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評估Model的RMES\n",
    "####### without scaler (depth=12, bin=64) with P_1, P_5, EMA30 MAC #########1\n",
    "#(RMSE) on test data = 0.754048    accuracy = 0.626374 final score: 14.720382982416249\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'requirement failed: Nothing has been added to this summarizer.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/app/bigdata/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/app/bigdata/spark/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o7097.evaluate.\n: java.lang.IllegalArgumentException: requirement failed: Nothing has been added to this summarizer.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.mllib.stat.MultivariateOnlineSummarizer.normL2(MultivariateOnlineSummarizer.scala:281)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr$lzycompute(RegressionMetrics.scala:65)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr(RegressionMetrics.scala:65)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.meanSquaredError(RegressionMetrics.scala:100)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.rootMeanSquaredError(RegressionMetrics.scala:109)\n\tat org.apache.spark.ml.evaluation.RegressionEvaluator.evaluate(RegressionEvaluator.scala:86)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-b5477c8d7415>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     evaluator = RegressionEvaluator(\n\u001b[1;32m      9\u001b[0m         labelCol=\"etf_close\", predictionCol=\"prediction\", metricName=\"rmse\")\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Root Mean Squared Error (RMSE) on test data = %g\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     evaluator = MulticlassClassificationEvaluator(\n",
      "\u001b[0;32m~/app/bigdata/spark/python/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/app/bigdata/spark/python/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \"\"\"\n\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0misLargerBetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/app/bigdata/spark/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/app/bigdata/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'requirement failed: Nothing has been added to this summarizer.'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator\n",
    "if DoEval:\n",
    "    tetf_dt_eval = tetfp_dt2.filter(\"etf_date >= '\" + predict_start_date + \"'\") \\\n",
    "        .select(\"etf_id\", \"etf_date\", \"etf_close\", \"close_price_raw\")\n",
    "    eval_dt = predict_res_final.join(tetf_dt_eval, [\"etf_id\", \"etf_date\"], \"inner\") \\\n",
    "        .withColumn(\"act_ud\", judge_up_down_pred(col(\"etf_close\"), col(\"close_price_raw\")))\n",
    "    eval_dt.cache()\n",
    "    #eval_dt.orderBy(\"etf_id\", \"etf_date\", ascending=True).show(10)\n",
    "\n",
    "    evaluator = RegressionEvaluator(\n",
    "        labelCol=\"etf_close\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(eval_dt)\n",
    "    print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"act_ud\", predictionCol=\"pred_ud\", metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(eval_dt)\n",
    "    print(\"accuracy = %g \" % accuracy)\n",
    "    \n",
    "    #define method to evaluate by ETF way\n",
    "    final_res = eval_dt.select(\"etf_id\", \"etf_date\", \"prediction\", \"pred_ud\", \"etf_close\", \"act_ud\") \\\n",
    "        .orderBy(\"etf_date\", \"etf_id\").collect()\n",
    "    weights = [0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "    judge_score = 0.0\n",
    "    for row in final_res:\n",
    "        #(up equal*0.5+((act_p - abs(pred_p - act_p))/act_p)*0.5)*weight\n",
    "        act_price = row[\"etf_close\"]\n",
    "        pred_price = row[\"prediction\"]\n",
    "        eidx = next_date_range.index(row[\"etf_date\"])\n",
    "        judge_score = judge_score + \\\n",
    "            ((0.5 if row[\"pred_ud\"] == row[\"act_ud\"] else 0) + \\\n",
    "             ((act_price-abs(pred_price-act_price))/act_price)*0.5)*weights[eidx]\n",
    "    print(\"final score: \", judge_score)\n",
    "#numtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_res = predict_res_final.orderBy(\"etf_id\", \"etf_date\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#export to pandas\n",
    "etf_ids = []\n",
    "mon_ud = []\n",
    "mon_price = [] \n",
    "tue_ud = []\n",
    "tue_price = []\n",
    "wed_ud = []\n",
    "wed_price = []\n",
    "thu_ud = []\n",
    "thu_price = []\n",
    "fri_ud = []\n",
    "fri_price = []\n",
    "\n",
    "def encode_ud(oper_ud):\n",
    "    if oper_ud == 0.0:\n",
    "        return 0\n",
    "    elif oper_ud == 1.0:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "for row in final_res:\n",
    "    etf_id = row[\"etf_id\"]\n",
    "    if etf_id not in etf_ids:\n",
    "        etf_ids.append(etf_id)\n",
    "    etf_date = row[\"etf_date\"]\n",
    "    eidx = next_date_range.index(row[\"etf_date\"])\n",
    "    if eidx == 0:\n",
    "        mon_ud.append(encode_ud(row[\"pred_ud\"]))\n",
    "        mon_price.append(row[\"prediction\"])\n",
    "    elif eidx == 1:\n",
    "        tue_ud.append(encode_ud(row[\"pred_ud\"]))\n",
    "        tue_price.append(row[\"prediction\"])\n",
    "    elif eidx == 2:\n",
    "        wed_ud.append(encode_ud(row[\"pred_ud\"]))\n",
    "        wed_price.append(row[\"prediction\"])\n",
    "    elif eidx == 3:\n",
    "        thu_ud.append(encode_ud(row[\"pred_ud\"]))\n",
    "        thu_price.append(row[\"prediction\"])\n",
    "    elif eidx == 4:\n",
    "        fri_ud.append(encode_ud(row[\"pred_ud\"]))\n",
    "        fri_price.append(row[\"prediction\"])\n",
    "        \n",
    "if len(mon_ud) == 0:\n",
    "    mon_ud = list(0 for i in range(0,len(etf_ids)))\n",
    "    mon_price = list(0.0 for i in range(0,len(etf_ids)))\n",
    "if len(tue_ud) == 0:\n",
    "    tue_ud = list(0 for i in range(0,len(etf_ids)))\n",
    "    tue_price = list(0.0 for i in range(0,len(etf_ids)))\n",
    "if len(wed_ud) == 0:\n",
    "    wed_ud = list(0 for i in range(0,len(etf_ids)))\n",
    "    wed_price = list(0.0 for i in range(0,len(etf_ids)))\n",
    "if len(thu_ud) == 0:\n",
    "    thu_ud = list(0 for i in range(0,len(etf_ids)))\n",
    "    thu_price = list(0.0 for i in range(0,len(etf_ids)))\n",
    "if len(fri_ud) == 0:\n",
    "    fri_ud = list(0 for i in range(0,len(etf_ids)))\n",
    "    fri_price = list(0.0 for i in range(0,len(etf_ids)))\n",
    "    \n",
    "dic = {\"ETFid\": etf_ids, \n",
    "       \"Mon_ud\": mon_ud, \"Mon_cprice\": mon_price,\n",
    "       \"Tue_ud\": tue_ud, \"Tue_cprice\": tue_price,\n",
    "       \"Wed_ud\": wed_ud, \"Wed_cprice\": wed_price,\n",
    "       \"Thu_ud\": thu_ud, \"Thu_cprice\": thu_price,\n",
    "       \"Fri_ud\": fri_ud, \"Fri_cprice\": fri_price\n",
    "      }\n",
    "final_df = pd.DataFrame(data=dic)[['ETFid','Mon_ud','Mon_cprice','Tue_ud','Tue_cprice',\n",
    "                                  'Wed_ud','Wed_cprice','Thu_ud','Thu_cprice',\n",
    "                                  'Fri_ud','Fri_cprice']]\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fri_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(Path.replace(\"file:\",\"\") + \"/etf_price_pred.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ll = [46.92, 47.31, 47.0, 46.79, 46.49, 46.66, 47.0, 46.96, 47.0]\n",
    "print(list(0 for i in range(0,5)))\n",
    "ll = [46.92, 47.31, 47.0, 46.79, 46.49, 46.66]\n",
    "win_len=5\n",
    "print(ll[0])\n",
    "print(ll[0: -1])\n",
    "print(list(map(lambda x,y : x - y, ll[1:], ll[0: -1])))\n",
    "for x in ll[win_len:]:\n",
    "    print(x)\n",
    "ema = sum(ll[:win_len])/len(ll[:win_len])\n",
    "print(ema)\n",
    "for price in ll[win_len:]:\n",
    "    ema = (ema*(win_len-1)+price*2)/(win_len+1)\n",
    "tup1, tup2 = (1,2)\n",
    "print(tup1, ' ', tup2)\n",
    "tup = (3,4)\n",
    "print(tup[0], ' ', tup[1])\n",
    "list(range(0,2))\n",
    "test_dic = {}\n",
    "test_dic.update({\"0051\": assembler})\n",
    "print(test_dic['0051'])\n",
    "print([x for x in next_date_range if x not in ignore_dates])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

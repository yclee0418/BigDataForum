{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, udf, lag, rank, lit\n",
    "from pyspark.sql.window import Window\n",
    "import talib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "global DoEval #是否進行模型評估\n",
    "\n",
    "#DoEval = False\n",
    "# next_date_range = [\"20180611\", \"20180612\", \"20180613\", \"20180614\", \"20180615\"] #設定預測區間\n",
    "# next_date_range = [\"20180604\", \"20180605\", \"20180606\", \"20180607\", \"20180608\"] #設定預測區間\n",
    "next_date_range = [\"20180528\", \"20180529\", \"20180530\", \"20180531\", \"20180601\"] #設定預測區間\n",
    "# next_date_range = [\"20180521\", \"20180522\", \"20180523\", \"20180524\", \"20180525\"] #設定預測區間\n",
    "# DoEval = True\n",
    "# next_date_range = [\"20180514\", \"20180515\", \"20180516\", \"20180517\", \"20180518\"] #設定預測區間\n",
    "ignore_dates = [\"\"]#設定排除日(如端午節)\n",
    "#next_date_range = [\"20180507\", \"20180508\", \"20180509\", \"20180510\", \"20180511\"] #設定預測區間\n",
    "predict_start_date = next_date_range[0]\n",
    "\n",
    "if sc.master[0:5]==\"local\":\n",
    "#     Path = \"file:/c:/D Drive/work/bigData/pySpark/TBrain_Round2_DataSet_20180601\"\n",
    "    #Path = \"file:/Users/yungchuanlee/Documents/learn/AI競賽/ETF預測/TBrain_Round2_DataSet_20180511\"\n",
    "    Path = \"file:/home/hduser/app/bigdata/competition/etf/TBrain_Round2_DataSet_20180518\"\n",
    "else:\n",
    "    Path = \"hdfs://master:9000/user/hduser\"\n",
    "Path = \"file:/home/hduser/app/bigdata/competition/etf/TBrain_Round2_DataSet_20180608\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define alias of columns\n",
    "col_alias_etf= {\"代碼\":\"etf_id\", \"日期\": \"etf_date\", \"中文簡稱\": \"etf_name\", \"開盤價(元)\":\"etf_open\", \n",
    "            \"最高價(元)\":\"etf_high\", \"最低價(元)\":\"etf_low\", \"收盤價(元)\":\"etf_close\", \"成交張數(張)\":\"etf_count\"}\n",
    "col_alias_stock= {\"代碼\":\"stock_id\", \"日期\": \"stock_date\", \"中文簡稱\": \"stock_name\", \"開盤價(元)\":\"stock_open\", \n",
    "            \"最高價(元)\":\"stock_high\", \"最低價(元)\":\"stock_low\", \"收盤價(元)\":\"stock_close\", \"成交張數(張)\":\"stock_count\"}#udf\n",
    "def to_double(str_val):\n",
    "    return float(str_val.replace(\",\",\"\"))\n",
    "to_double=udf(to_double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def function to read data (因檔案格式都相同)\n",
    "def read_data(file_name, col_alias):\n",
    "    str_cols = [\"代碼\",\"日期\", \"中文簡稱\"]\n",
    "    raw_data = spark.read.option(\"encoding\", \"Big5\").csv(Path + \"/\" + file_name, header=True, sep=\",\")\n",
    "    print(\"Total \" + file_name + \" count: \" + str(raw_data.count()))\n",
    "    #rename cols and correct type \n",
    "    num_cols = [col_name for col_name in raw_data.columns if col_name not in str_cols]\n",
    "    final_data=raw_data.select( [col(str_col_name).alias(col_alias[str_col_name]) for str_col_name in str_cols] + \n",
    "                                  [to_double(col(num_col_name)).cast(\"double\").alias(col_alias[num_col_name]) for num_col_name in num_cols] )\n",
    "    final_data.printSchema()\n",
    "    final_data.show(5)\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting import tetfp.csv(台灣18檔ETF股價資料)...\n",
      "Total tetfp.csv count: 19575\n",
      "root\n",
      " |-- etf_id: string (nullable = true)\n",
      " |-- etf_date: string (nullable = true)\n",
      " |-- etf_name: string (nullable = true)\n",
      " |-- etf_open: double (nullable = true)\n",
      " |-- etf_high: double (nullable = true)\n",
      " |-- etf_low: double (nullable = true)\n",
      " |-- etf_close: double (nullable = true)\n",
      " |-- etf_count: double (nullable = true)\n",
      "\n",
      "+-------+--------+----------------+--------+--------+-------+---------+---------+\n",
      "| etf_id|etf_date|        etf_name|etf_open|etf_high|etf_low|etf_close|etf_count|\n",
      "+-------+--------+----------------+--------+--------+-------+---------+---------+\n",
      "|0050   |20130102|元大台灣50          |    54.0|   54.65|   53.9|     54.4|  16487.0|\n",
      "|0050   |20130103|元大台灣50          |    54.9|   55.05|  54.65|    54.85|  29020.0|\n",
      "|0050   |20130104|元大台灣50          |   54.85|   54.85|   54.4|     54.5|   9837.0|\n",
      "|0050   |20130107|元大台灣50          |   54.55|   54.55|   53.9|    54.25|   8910.0|\n",
      "|0050   |20130108|元大台灣50          |    54.0|    54.2|  53.65|     53.9|  12507.0|\n",
      "+-------+--------+----------------+--------+--------+-------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"starting import tetfp.csv(台灣18檔ETF股價資料)...\")\n",
    "tetfp_dt=read_data(\"tetfp.csv\", col_alias_etf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0050   ': 0,\n",
       " '0051   ': 1,\n",
       " '0052   ': 2,\n",
       " '0053   ': 3,\n",
       " '0054   ': 4,\n",
       " '0055   ': 5,\n",
       " '0056   ': 6,\n",
       " '0057   ': 7,\n",
       " '0058   ': 8,\n",
       " '0059   ': 9,\n",
       " '006201 ': 10,\n",
       " '006203 ': 11,\n",
       " '006204 ': 12,\n",
       " '006208 ': 13,\n",
       " '00690  ': 14,\n",
       " '00692  ': 15,\n",
       " '00701  ': 16,\n",
       " '00713  ': 17}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#取出etf的distinct id\n",
    "etf_ids = []\n",
    "etf_idx_dic = {}\n",
    "etf_idx = 0\n",
    "for row in tetfp_dt.select(\"etf_id\").distinct().orderBy(\"etf_id\").collect():\n",
    "    etf_ids.append(row[\"etf_id\"])\n",
    "    etf_idx_dic.update({row[\"etf_id\"]: etf_idx})\n",
    "    etf_idx += 1\n",
    "etf_idx_dic\n",
    "# etf_ids = ['0050   ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "etf_dic = {}\n",
    "for etfid in etf_ids:\n",
    "    export_dt = tetfp_dt.filter(\"etf_id='\" +etfid+ \"' and etf_date < '\" + predict_start_date + \"'\") \\\n",
    "        .orderBy(\"etf_id\", \"etf_date\", ascending=True)\n",
    "    export_pd = export_dt.toPandas()\n",
    "    etf_dic.update({etfid.strip(): export_pd})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions of TA lib\n",
    "#print(talib.get_functions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1322"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etf_dic[\"0050\"][\"etf_close\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def avg_list(p_list):\n",
    "    #計算數字list的平均值\n",
    "    return sum(p_list)/len(p_list)\n",
    "\n",
    "def get_feature_pre(curr_list):\n",
    "    #在feature前加上一個Nan後回傳, 第二個值則回傳原本的最後一個值作為下次的feature\n",
    "    if type(curr_list) is pd.Series:\n",
    "        rtn_list = curr_list.tolist()\n",
    "    else:\n",
    "        rtn_list = curr_list\n",
    "    rtn_list = [None] + rtn_list\n",
    "    return (rtn_list[:-1], [rtn_list[-1]])\n",
    "\n",
    "def checkNan(num):\n",
    "    if num == None:\n",
    "        return True\n",
    "    elif math.isnan(num):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def calculate_n_bias(close_price, ma):\n",
    "#     (close price - MA)/MA\n",
    "    if checkNan(close_price) or checkNan(ma):\n",
    "        return None\n",
    "    else:\n",
    "        return (close_price - ma)/ma\n",
    "    \n",
    "def calculate_n_bias_list(close_ser, ma_list):\n",
    "    #calculate n bias (20 days)\n",
    "    bias_list = []\n",
    "    for cprice, ma in zip(close_ser.tolist(), ma_list):\n",
    "        bias_list.append(calculate_n_bias(cprice, ma))\n",
    "    return bias_list\n",
    "\n",
    "def calculate_bias_ma_list(ma3_list, ma6_list):\n",
    "    #calculate bias_ma = (ma3-ma6)/ma6\n",
    "    bias_list = []\n",
    "    for ma3, ma6 in zip(ma3_list, ma6_list):\n",
    "        bias_list.append(calculate_n_bias(ma3, ma6))\n",
    "    return bias_list\n",
    "\n",
    "def calculate_bias_3_6_list(bias3_list, bias6_list):\n",
    "    #bias_3_6 = bias3 - bias6\n",
    "    bias_list = []\n",
    "    for bias3, bias6 in zip(bias3_list, bias6_list):\n",
    "        if checkNan(bias3) or checkNan(bias6):\n",
    "            bias_list.append(None)\n",
    "        else:\n",
    "            bias_list.append(bias3 - bias6)\n",
    "    return bias_list\n",
    "\n",
    "def calculate_dif(ema12_list, ema26_list):\n",
    "    #差離值DIF = 12日EMA - 26日EMA \n",
    "    dif_list = []\n",
    "    for ema12, ema26 in zip(ema12_list, ema26_list):\n",
    "        if checkNan(ema12) or checkNan(ema26):\n",
    "            dif_list.append(None)\n",
    "        else:\n",
    "            dif_list.append(ema12 - ema26)\n",
    "    return dif_list\n",
    "def calculate_slope(cur_list, prev_list):\n",
    "    slope_list = []\n",
    "    for cur, prev in zip(cur_list, prev_list):\n",
    "        if checkNan(cur) or checkNan(prev):\n",
    "            slope_list.append(None)\n",
    "        else:\n",
    "            slope_list.append((cur-prev)/prev)\n",
    "    return slope_list\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import itertools\n",
    "def create_feature(etf_pd, next_date, scaler_dic):\n",
    "    close_ser = etf_pd[\"etf_close\"]\n",
    "    #EMA\n",
    "    ema3 = talib.EMA(close_ser,timeperiod=3)\n",
    "    ema3_prev, _ = get_feature_pre(ema3)\n",
    "    ema3_slope = calculate_slope(ema3, ema3_prev)\n",
    "    ema5 = talib.EMA(close_ser,timeperiod=5)\n",
    "    ema5_prev, _ = get_feature_pre(ema5)\n",
    "    ema5_slope = calculate_slope(ema5, ema5_prev)\n",
    "    ema10 = talib.EMA(close_ser,timeperiod=10)\n",
    "    ema10_prev, _ = get_feature_pre(ema10)\n",
    "    ema10_slope = calculate_slope(ema10, ema10_prev)\n",
    "    ema20 = talib.EMA(close_ser,timeperiod=20)\n",
    "    ema30 = talib.EMA(close_ser,timeperiod=30)\n",
    "    \n",
    "    #make diff\n",
    "    close_ser_prev, _ = get_feature_pre(close_ser)\n",
    "    close_diff = calculate_dif(close_ser.tolist(), close_ser_prev)\n",
    "    \n",
    "    close_diff_2, _ = get_feature_pre(close_diff)\n",
    "    close_diff_3, _ = get_feature_pre(close_diff_2)\n",
    "    #BIAS\n",
    "    #nBIAS -3, 6, 20 => (close price - MA)/MA   ,Paper 建議用20日MA\n",
    "    ma3 = talib.MA(np.array(close_ser), timeperiod=3)\n",
    "    ma6 = talib.MA(np.array(close_ser), timeperiod=6)\n",
    "    ma20 = talib.MA(np.array(close_ser), timeperiod=20)\n",
    "    nbias3 = calculate_n_bias_list(close_ser, ma3)\n",
    "    nbias6 = calculate_n_bias_list(close_ser, ma6)\n",
    "    nbias20 = calculate_n_bias_list(close_ser, ma20)\n",
    "    \n",
    "    #BIAS_ma = (ma3-ma6)/ma6\n",
    "    bias_ma = calculate_bias_ma_list(ma3, ma6)\n",
    "    \n",
    "    #bias_3_6 = bias3 - bias6\n",
    "    bias_3_6 = calculate_bias_3_6_list(nbias3, nbias6)\n",
    "    \n",
    "    #KD --> only use STOCHRSI\n",
    "    k, d = talib.STOCHRSI(close_ser, timeperiod=9, fastk_period=3, fastd_period=3, fastd_matype=0)\n",
    "    \n",
    "    #差離值DIF = 12日EMA - 26日EMA \n",
    "    ema12 = talib.EMA(close_ser,timeperiod=12)\n",
    "    ema26 = talib.EMA(close_ser,timeperiod=26)\n",
    "    dif = calculate_dif(ema12, ema26)\n",
    "    \n",
    "    #MACD\n",
    "#     macd1, macdsignal1, macdhist1 = talib.MACD(close_ser, fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "    macd1, macdsignal1, macdhist1 = talib.MACDEXT(close_ser, fastperiod=12, fastmatype=0, \n",
    "                                               slowperiod=26, slowmatype=0, signalperiod=9, signalmatype=0)\n",
    "#     macd1, macdsignal1, macdhist1 = talib.MACDFIX(close_ser, signalperiod=9)\n",
    "    #RSI\n",
    "    rsi = talib.RSI(close_ser, timeperiod=10)\n",
    "    \n",
    "    #CMO\n",
    "    cmo = talib.CMO(close_ser, timeperiod=14)\n",
    "    \n",
    "    #ROCR - Rate of change ratio: (price/prevPrice)\n",
    "    #rocr = talib.ROCR(close_ser, timeperiod=10)\n",
    "    #ROC - Rate of change : ((price/prevPrice)-1)*100\n",
    "    rocr = talib.ROC(close_ser, timeperiod=10)\n",
    "    \n",
    "    #PPO - Percentage Price Oscillator\n",
    "    ppo = talib.PPO(close_ser, fastperiod=12, slowperiod=26, matype=0)\n",
    "    #APO - Absolute Price Oscillator\n",
    "    apo = talib.APO(close_ser, fastperiod=12, slowperiod=26, matype=0)\n",
    "    \n",
    "    #MOM - Momentum\n",
    "    mom = talib.MOM(close_ser, timeperiod=10)\n",
    "    #TRIX - 1-day Rate-Of-Change (ROC) of a Triple Smooth EMA\n",
    "    trix = talib.TRIX(close_ser, timeperiod=30)\n",
    "    #作標準化\n",
    "    etfid = etf_pd[\"etf_id\"].tolist()[0]\n",
    "    tmp_pd = pd.DataFrame({\"etf_id\": etfid, \"etf_date\": etf_pd[\"etf_date\"], \"etf_close\": close_ser, \"close_dif\": close_diff,  \n",
    "#                            \"close_diff_1\": close_diff, \"close_diff_2\": close_diff_2, \"close_diff_3\": close_diff_3,\n",
    "                           \"ema3\":ema3, \"ema5\": ema5, \"ema10\": ema10, \"ema20\": ema20, \"ema30\" : ema30,\n",
    "                           \"ema3_slope\": ema3_slope, \"ema5_slope\": ema5_slope, \"ema10_slope\": ema10_slope,\n",
    "                          \"nbias3\": nbias3, \"nbias6\": nbias6, \"nbias20\": nbias20, \"bias_ma\": bias_ma,\n",
    "                          \"bias_3_6\": bias_3_6, \"k\": k, \"d\": d, \"dif\": dif,\n",
    "                          #\"macd\": macd, \"macdsignal\": macdsignal, \"macdhist\": macdhist,\n",
    "                          \"macd1\": macd1, \"macdsignal1\": macdsignal1, \"macdhist1\": macdhist1,\n",
    "                          \"rsi\": rsi, \"cmo\": cmo, \"rocr\": rocr, \"ppo\": ppo}).dropna(how='any')\n",
    "    tmp_pd = tmp_pd[tmp_pd['etf_date'] > '20140101']\n",
    "    #exclude outlier\n",
    "#     for col in [c for c in tmp_pd.columns.values if c not in [\"etf_id\", \"etf_date\"]]:\n",
    "#         tmp_pd = tmp_pd[np.abs(tmp_pd[col] - tmp_pd[col].mean()) <= 3.0*tmp_pd[col].std()] #只取3倍以內標準差資料\n",
    "    \n",
    "    std_dic = {}\n",
    "    for col in [c for c in tmp_pd.columns.values if c not in [\"etf_id\", \"etf_date\"]]:\n",
    "#         scaler_key = etfid.strip() + '-' + col\n",
    "        scaler_key = col\n",
    "        if scaler_key in scaler_dic:\n",
    "            scaler = scaler_dic[scaler_key]\n",
    "            merged = list(itertools.chain.from_iterable(scaler.transform(tmp_pd[[col]])))                \n",
    "#             merged = tmp_pd[col]\n",
    "        else:\n",
    "            scaler = MinMaxScaler()\n",
    "            merged = list(itertools.chain.from_iterable(scaler.fit_transform(tmp_pd[[col]])))\n",
    "            merged = tmp_pd[col]\n",
    "            #保留etf_close的scaler在後續使用\n",
    "            scaler_dic.update({scaler_key: scaler})\n",
    "        std_dic.update({col: merged})\n",
    "    \n",
    "    #作出目前的feature table及預測用的feature table\n",
    "    ema3, next_ema3 = get_feature_pre(std_dic[\"ema3\"])\n",
    "    ema3_slope, next_ema3_slope = get_feature_pre(std_dic[\"ema3_slope\"])\n",
    "    ema5, next_ema5 = get_feature_pre(std_dic[\"ema5\"])\n",
    "    ema5_slope, next_ema5_slope = get_feature_pre(std_dic[\"ema5_slope\"])\n",
    "    ema10, next_ema10 = get_feature_pre(std_dic[\"ema10\"])\n",
    "    ema10_slope, next_ema10_slope = get_feature_pre(std_dic[\"ema10_slope\"])\n",
    "    ema20, next_ema20 = get_feature_pre(std_dic[\"ema20\"])\n",
    "    ema30, next_ema30 = get_feature_pre(std_dic[\"ema30\"])\n",
    "    \n",
    "    nbias3, next_nbias3 = get_feature_pre(std_dic[\"nbias3\"])\n",
    "    nbias6, next_nbias6 = get_feature_pre(std_dic[\"nbias6\"])\n",
    "    nbias20, next_nbias20 = get_feature_pre(std_dic[\"nbias20\"])\n",
    "    \n",
    "    bias_ma, next_bias_ma = get_feature_pre(std_dic[\"bias_ma\"])\n",
    "    bias_3_6, next_bias_3_6 = get_feature_pre(std_dic[\"bias_3_6\"])\n",
    "    k, next_k = get_feature_pre(std_dic['k'])\n",
    "    d, next_d = get_feature_pre(std_dic['d'])\n",
    "    dif, next_dif = get_feature_pre(std_dic['dif'])\n",
    "    \n",
    "    #macd, next_macd = get_feature_pre(std_dic['macd'])\n",
    "    #macdsignal, next_macdsignal = get_feature_pre(std_dic['macdsignal'])\n",
    "    #macdhist, next_macdhist = get_feature_pre(std_dic['macdhist'])\n",
    "    macd1, next_macd1 = get_feature_pre(std_dic['macd1'])\n",
    "    macdsignal1, next_macdsignal1 = get_feature_pre(std_dic['macdsignal1'])\n",
    "    macdhist1, next_macdhist1 = get_feature_pre(std_dic['macdhist1'])\n",
    "                                               \n",
    "    rsi, next_rsi = get_feature_pre(std_dic['rsi'])\n",
    "    cmo, next_cmo = get_feature_pre(std_dic['cmo'])\n",
    "    rocr, next_rocr = get_feature_pre(std_dic['rocr'])\n",
    "    ppo, next_ppo = get_feature_pre(std_dic['ppo'])\n",
    "    \n",
    "#     close_diff_1, next_close_diff_1 = get_feature_pre(std_dic['close_diff_1'])\n",
    "#     close_diff_2, next_close_diff_2 = get_feature_pre(std_dic['close_diff_2'])\n",
    "#     close_diff_3, next_close_diff_3 = get_feature_pre(std_dic['close_diff_3'])\n",
    "    \n",
    "    #\"apo\": apo, \"mom\": mom, \"trix\": trix\n",
    "#     apo, next_apo = get_feature_pre(std_dic['apo'])\n",
    "#     mom, next_mom = get_feature_pre(std_dic['mom'])\n",
    "#     trix, next_trix = get_feature_pre(std_dic['trix'])\n",
    "#     print(len(std_dic['etf_close']), ' ', len(close_diff_1), ' ', len(close_diff_2), ' ' , len(rsi))\n",
    "    \n",
    "    rtn_pd = pd.DataFrame({\"etf_id\": etfid, \"idx\": etf_idx_dic[etfid], \"etf_date\": tmp_pd[\"etf_date\"], \n",
    "                           \"etf_close\": std_dic[\"etf_close\"], \"close_dif\": std_dic['close_dif'], \n",
    "                           'ema3': ema3, \"ema5\": ema5, \"ema10\": ema10, \"ema20\": ema20, \"ema30\" : ema30,\n",
    "                          \"ema3_slope\": ema3_slope, \"ema5_slope\": ema5_slope, \"ema10_slope\": ema10_slope,\n",
    "                           \"nbias3\": nbias3, \"nbias6\": nbias6, \"nbias20\": nbias20, \"bias_ma\": bias_ma,\n",
    "                          \"bias_3_6\": bias_3_6, \"k\": k, \"d\": d, \"dif\": dif,\n",
    "                          #\"macd\": macd, \"macdsignal\": macdsignal, \"macdhist\": macdhist,\n",
    "                           \"macd1\": macd1, \"macdsignal1\": macdsignal1, \"macdhist1\": macdhist1,\n",
    "                          \"rsi\": rsi, \"cmo\": cmo, \"rocr\": rocr, \"ppo\": ppo}).dropna(how='any')\n",
    "    #col = 'close_dif'\n",
    "    #rtn_pd = rtn_pd[np.abs(rtn_pd[col] - rtn_pd[col].mean()) <= 3.0*rtn_pd[col].std()] #只取3倍以內標準差資料\n",
    "    \n",
    "    next_pd = pd.DataFrame({\"etf_id\": etfid, \"idx\": etf_idx_dic[etfid], \"etf_date\": next_date, \n",
    "                            'etf_close': -1.0, \"close_dif\": -1.0, \n",
    "                           \"ema3\": next_ema3, \"ema5\": next_ema5, \"ema10\": next_ema10, \"ema20\": next_ema20, \"ema30\" : next_ema30,\n",
    "                          \"ema3_slope\": next_ema3_slope, \"ema5_slope\": next_ema5_slope, \"ema10_slope\": next_ema10_slope,\n",
    "                            \"nbias3\": next_nbias3, \"nbias6\": next_nbias6, \"nbias20\": next_nbias20, \"bias_ma\": next_bias_ma,\n",
    "                          \"bias_3_6\": next_bias_3_6, \"k\": next_k, \"d\": next_d, \"dif\": next_dif,\n",
    "                          #\"macd\": next_macd, \"macdsignal\": next_macdsignal, \"macdhist\": next_macdhist,\n",
    "                            \"macd1\": next_macd1, \"macdsignal1\": next_macdsignal1, \"macdhist1\": next_macdhist1,\n",
    "                          \"rsi\": next_rsi, \"cmo\": next_cmo, \"rocr\": next_rocr, \"ppo\": next_ppo})\n",
    "    \n",
    "    return (rtn_pd, next_pd, scaler_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.398776026640185"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = etf_dic[\"0050\"][[\"etf_close\"]]\n",
    "# print(val[\"etf_close\"].tolist())\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import itertools\n",
    "scaler = StandardScaler()\n",
    "scaler.fit_transform(val)\n",
    "#merged = list(itertools.chain.from_iterable(scaler.fit_transform(val)))\n",
    "#merged\n",
    "scaler.inverse_transform([-1.44880072])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create feature pandas\n",
    "etf_pd_dic = {}\n",
    "next_pd_dic = {}\n",
    "scaler_dic = {}\n",
    "for etfid in etf_ids:\n",
    "    etf_pd, next_pd, scalers = create_feature(etf_dic[etfid.strip()], predict_start_date, {})\n",
    "    etf_pd_dic.update({etfid.strip() : etf_pd})\n",
    "    next_pd_dic.update({etfid.strip() : next_pd})\n",
    "    scaler_dic.update(scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1075.000000\n",
       "mean       69.992698\n",
       "std         7.942772\n",
       "min        55.800000\n",
       "25%        63.800000\n",
       "50%        68.950000\n",
       "75%        74.525000\n",
       "max        88.300000\n",
       "Name: etf_close, dtype: float64"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etf_pd_dic[\"0050\"]['etf_close'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#etf_pd_dic[\"0054\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#etf_pd_dic[\"00692\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import itertools\n",
    "# pd_0050=etf_pd_dic[\"0051\"]\n",
    "# pd_0050=pd_0050[pd_0050.etf_date > '20180101']\n",
    "# scaler = MinMaxScaler()\n",
    "# pd_0050['close_dif']=list(itertools.chain.from_iterable(scaler.fit_transform(pd_0050[['close_dif']])))\n",
    "# # pd_0050[['close_dif']].plot()\n",
    "# for col in list(pd_0050.columns.values): #[\"bias_3_6\", \"ema5_slope\", 'ema10_slope']:\n",
    "#     pd_0050[col]=list(itertools.chain.from_iterable(scaler.fit_transform(pd_0050[[col]])))\n",
    "#     pd_0050[['close_dif',col]].plot()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bias_3_6', 'bias_ma', 'cmo', 'd', 'dif', 'ema10', 'ema10_slope', 'ema20', 'ema3', 'ema30', 'ema3_slope', 'ema5', 'ema5_slope', 'k', 'macd1', 'macdhist1', 'macdsignal1', 'nbias20', 'nbias3', 'nbias6', 'ppo', 'rocr', 'rsi']\n"
     ]
    }
   ],
   "source": [
    "non_feature_list = ['etf_id', 'etf_date', 'close_dif', 'etf_close', 'idx']\n",
    "feature_cols = [col for col in list(etf_pd_dic[\"0050\"].columns.values) if col not in non_feature_list]\n",
    "# feature_cols = ['ema5']\n",
    "print(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dt count: (15801, 26)\n",
      "next_pd_dic count: 18 * (1, 26)\n"
     ]
    }
   ],
   "source": [
    "train_dt_org = pd.concat([etf_pd_dic[etfid.strip()] for etfid in etf_ids])\n",
    "std_dic = {}\n",
    "for col in [c for c in train_dt_org.columns.values if c not in [\"etf_id\", \"etf_date\"]]:\n",
    "        scaler_key = col\n",
    "        if scaler_key in scaler_dic:\n",
    "            scaler = scaler_dic[scaler_key]\n",
    "            merged = list(itertools.chain.from_iterable(scaler.transform(train_dt_org[[col]])))                \n",
    "        else:\n",
    "            scaler = MinMaxScaler()\n",
    "            merged = list(itertools.chain.from_iterable(scaler.fit_transform(train_dt_org[[col]])))\n",
    "            #保留etf_close的scaler在後續使用\n",
    "            scaler_dic.update({scaler_key: scaler})\n",
    "        std_dic.update({col: merged})\n",
    "train_dt = pd.DataFrame(std_dic)\n",
    "print('train_dt count:', train_dt.shape)\n",
    "\n",
    "for etfid in etf_ids:\n",
    "    std_dic = {}\n",
    "    next_pd = next_pd_dic[etfid.strip()]\n",
    "    for col in [c for c in next_pd.columns.values if c not in [\"etf_id\", \"etf_date\"]]:\n",
    "        scaler = scaler_dic[col]\n",
    "        merged = list(itertools.chain.from_iterable(scaler.transform(next_pd[[col]])))\n",
    "        std_dic.update({col: merged})\n",
    "    next_pd_dic.update({etfid.strip() : pd.DataFrame(std_dic)})\n",
    "print('next_pd_dic count:', len(next_pd_dic.keys()), '*' , next_pd_dic['0050'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bias_3_6', 'ema20', 'ema30', 'macd1']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list('bias_3_6,ema20,ema30,macd1'.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "def trainModel(train_dt, feature_cols, f_type):\n",
    "    train_x1 = train_dt[feature_cols].values\n",
    "    if f_type == '1':\n",
    "        #model for close_dif\n",
    "        train_y1 = train_dt[\"close_dif\"].values\n",
    "        #$$$ parameter\n",
    "        model1 = xgb.XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75, \n",
    "            colsample_bytree=1, max_depth=68, n_jobs=4, min_child_weight=0 )\n",
    "        model1.fit(train_x1, train_y1)\n",
    "        return model1\n",
    "    else:\n",
    "        #model for etf_close\n",
    "        train_y2 = train_dt[\"etf_close\"].values\n",
    "        model2 = xgb.XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75, \n",
    "            colsample_bytree=1, max_depth=68, n_jobs=4, min_child_weight=0 )\n",
    "        model2.fit(train_x1, train_y2)\n",
    "        return model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20180525'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_0050=etf_dic[\"0050\"]\n",
    "prev_date = pd_0050.loc[~pd_0050[\"etf_date\"].isin(next_date_range)][\"etf_date\"].max()\n",
    "prev_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, DoubleType, IntegerType\n",
    "#計算上或下的值(udf)\n",
    "def judge_up_down_pred_native(curr_price, prev_close_price):\n",
    "    if checkNan(prev_close_price):\n",
    "        return 0.0\n",
    "    else:\n",
    "        prev_price = prev_close_price\n",
    "        if curr_price == prev_price:\n",
    "            return 0.0\n",
    "        elif curr_price > prev_price:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 2.0\n",
    "judge_up_down_pred=udf(judge_up_down_pred_native, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------------+--------+--------+-------+---------+---------+----------+------+\n",
      "| etf_id|etf_date|        etf_name|etf_open|etf_high|etf_low|etf_close|etf_count|prev_close|act_ud|\n",
      "+-------+--------+----------------+--------+--------+-------+---------+---------+----------+------+\n",
      "|00701  |20180528|   國泰臺灣低波動30    |   21.29|   21.29|  21.23|    21.24|     59.0|     21.21|   1.0|\n",
      "|00701  |20180529|   國泰臺灣低波動30    |    21.2|    21.2|  21.08|    21.08|     94.0|     21.24|   2.0|\n",
      "|00701  |20180530|   國泰臺灣低波動30    |    21.0|    21.0|  20.78|    20.78|     42.0|     21.08|   2.0|\n",
      "|00701  |20180531|   國泰臺灣低波動30    |    20.9|    20.9|  20.85|    20.87|      6.0|     20.78|   1.0|\n",
      "|00701  |20180601|   國泰臺灣低波動30    |   20.89|   21.03|  20.89|    21.03|    101.0|     20.87|   1.0|\n",
      "|0051   |20180528|元大中型100         |   33.85|   33.91|  33.85|    33.87|      9.0|     33.66|   1.0|\n",
      "|0051   |20180529|元大中型100         |   33.98|   33.98|  33.97|    33.97|     10.0|     33.87|   1.0|\n",
      "|0051   |20180530|元大中型100         |   33.84|   33.84|  33.47|    33.47|     25.0|     33.97|   2.0|\n",
      "|0051   |20180531|元大中型100         |   33.77|   33.77|  33.71|    33.71|      4.0|     33.47|   1.0|\n",
      "|0051   |20180601|元大中型100         |   33.69|   33.94|  33.69|    33.94|     12.0|     33.71|   1.0|\n",
      "|0057   |20180528|富邦摩台            |    50.4|   50.45|   50.4|    50.45|     37.0|      50.4|   1.0|\n",
      "|0057   |20180529|富邦摩台            |    50.4|    50.4|  50.05|    50.05|      7.0|     50.45|   2.0|\n",
      "|0057   |20180530|富邦摩台            |   49.65|   49.65|  49.31|    49.39|     11.0|     50.05|   2.0|\n",
      "|0057   |20180531|富邦摩台            |   49.41|   49.68|  49.41|    49.68|     43.0|     49.39|   1.0|\n",
      "|0057   |20180601|富邦摩台            |   49.84|    49.9|  49.84|     49.9|      2.0|     49.68|   1.0|\n",
      "|006203 |20180528|元大MSCI台灣        |   38.16|   38.31|  38.16|    38.31|      2.0|     38.25|   1.0|\n",
      "|006203 |20180529|元大MSCI台灣        |    38.2|    38.2|   38.2|     38.2|      1.0|     38.31|   2.0|\n",
      "|006203 |20180530|元大MSCI台灣        |   37.88|   37.88|  37.65|    37.65|     22.0|      38.2|   2.0|\n",
      "|006203 |20180531|元大MSCI台灣        |   37.63|   37.63|  37.63|    37.63|      1.0|     37.65|   2.0|\n",
      "|006203 |20180601|元大MSCI台灣        |   37.78|   37.79|  37.78|    37.79|     11.0|     37.63|   1.0|\n",
      "+-------+--------+----------------+--------+--------+-------+---------+---------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#declare previous row windows\n",
    "wsSpec_etf = Window.partitionBy('etf_id').orderBy('etf_date') #time window for normal case\n",
    "#eval dt for evluate score\n",
    "tetf_dt_eval = tetfp_dt.filter(tetfp_dt.etf_date.isin([prev_date] + next_date_range)) \\\n",
    "    .withColumn(\"prev_close\", lag(tetfp_dt.etf_close).over(wsSpec_etf)) \n",
    "tetf_dt_eval =  tetf_dt_eval.withColumn(\"act_ud\", judge_up_down_pred(tetf_dt_eval.etf_close, tetf_dt_eval.prev_close))\n",
    "tetf_dt_eval = tetf_dt_eval.filter(tetf_dt_eval.etf_date.isin(next_date_range))\n",
    "tetf_dt_eval.cache()\n",
    "tetf_dt_eval.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tetf_pd_eval = tetf_dt_eval.toPandas()\n",
    "# tetf_pd_eval.loc[(tetf_pd_eval['etf_id'] == '006203 ') & (tetf_pd_eval['etf_date'] == '20180522')]['etf_close'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score\n",
    "def doEvaluate(predict_res_final, tetf_pd_eval):\n",
    "    etf_close_list = []\n",
    "    act_ud_list = []\n",
    "    prediction_list = []\n",
    "    pred_ud_list = []\n",
    "    acc_list = []\n",
    "    weights = [0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "    judge_score = 0.0\n",
    "    for row in predict_res_final:\n",
    "        eid = row['etf_id']\n",
    "        edate = row['etf_date']\n",
    "        if edate not in next_date_range:\n",
    "            continue\n",
    "        etf_eval = tetf_pd_eval.loc[(tetf_pd_eval['etf_id'] == eid) & (tetf_pd_eval['etf_date'] == edate)]\n",
    "        etf_close = etf_eval['etf_close'].values[0]\n",
    "        act_ud = etf_eval['act_ud'].values[0]\n",
    "        prediction = row['prediction']\n",
    "        pred_ud = row['pred_ud']\n",
    "        #etf score\n",
    "        eidx = next_date_range.index(edate)\n",
    "        judge_score = judge_score + \\\n",
    "            ((0.5 if pred_ud == act_ud else 0.0) + \\\n",
    "             ((etf_close-abs(prediction-etf_close))/etf_close)*0.5)*weights[eidx]\n",
    "        etf_close_list.append(etf_close)\n",
    "        act_ud_list.append(act_ud)\n",
    "        prediction_list.append(prediction)\n",
    "        pred_ud_list.append(pred_ud)\n",
    "        acc_list.append(1.0 if pred_ud == act_ud else 0.0)\n",
    "    #evaluate accuracy_score\n",
    "    accuracy = accuracy_score(act_ud_list, pred_ud_list, normalize=True)\n",
    "#     print(act_ud_list)\n",
    "#     print(pred_ud_list)\n",
    "    rmse = mean_squared_error(etf_close_list, prediction_list)\n",
    "    return (rmse, accuracy, judge_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['bias_3_6', 'bias_ma', 'close_dif', 'cmo', 'd', 'dif', 'ema10', 'ema10_slope', 'ema20', 'ema3', 'ema30', 'ema3_slope', 'ema5', 'ema5_slope', 'etf_close', 'k', 'macd1', 'macdhist1', 'macdsignal1', 'nbias20', 'nbias3', 'nbias6', 'ppo', 'rocr', 'rsi', 'idx'])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_7percent(prev_close, prediction):\n",
    "    dif = prediction - prev_close\n",
    "    pct = 0.07\n",
    "    dif_max = prev_close * pct\n",
    "    rtn_val = prediction\n",
    "    if abs(dif) > dif_max:\n",
    "        if dif > 0:\n",
    "            rtn_val = prev_close + dif_max\n",
    "        else:\n",
    "            rtn_val = prev_close - dif_max\n",
    "    return rtn_val\n",
    "        \n",
    "def doPredict1(next_date_range, model1, etf_dic, next_pd_dic, scaler_dic, feature_cols):\n",
    "    predict_res_final_1 = []\n",
    "    dummy_date = '20999999'\n",
    "    for date in next_date_range:\n",
    "        for eid in etf_ids:\n",
    "            #do predict\n",
    "            pred_data = next_pd_dic[eid.strip()][feature_cols].values\n",
    "            pred_res = model1.predict(data=pred_data)\n",
    "#             print(pred_res)\n",
    "\n",
    "            etf_dt = etf_dic[eid.strip()]\n",
    "            prev_dt = etf_dt.loc[(etf_dt[\"etf_date\"] < date) ].iloc[-1]\n",
    "#             print('id of prev_dt: ', eid, ', date: ' ,prev_dt[\"etf_date\"])\n",
    "            prev_close = prev_dt[\"etf_close\"]\n",
    "            scaler = scaler_dic['close_dif']\n",
    "            prediction = prev_close + scaler.inverse_transform(pred_res[0])[0][0]\n",
    "            prediction = max_7percent(prev_close, prediction)\n",
    "            #取出所有預測結果作合併，以進行後續成績計算\n",
    "            predict_res_final_1.append({\n",
    "                'etf_id': eid, 'etf_date': date,\n",
    "                'prediction': prediction, 'pred_ud': judge_up_down_pred_native(prediction, prev_close)\n",
    "            })\n",
    "\n",
    "            if (date != next_date_range[-1]) and (date != dummy_date):\n",
    "                #作出新的next_pd\n",
    "                new_dic = {'etf_id': [eid], 'etf_date': [date], 'etf_name': [''], \n",
    "                           'etf_open': [0.0],  'etf_high': [0.0], 'etf_low': [0.0], 'etf_close': [prediction], 'etf_count': [0.0]}\n",
    "                new_pd = pd.DataFrame(data=new_dic)[['etf_id','etf_date','etf_name', 'etf_open', 'etf_high', \n",
    "                                                          'etf_low', 'etf_close', 'etf_count']]\n",
    "                #print(new_pd)\n",
    "                etf_dt=etf_dt.append(new_pd, ignore_index=True)\n",
    "                next_date = next_date_range[next_date_range.index(date)+1]\n",
    "                etf_pd2, next_pd2, _ = create_feature(etf_dt, next_date, scaler_dic)\n",
    "                etf_dic.update({eid.strip(): etf_dt})\n",
    "                next_pd_dic.update({eid.strip(): next_pd2})\n",
    "    return predict_res_final_1\n",
    "    \n",
    "def doPredict2(next_date_range, model2, etf_dic, next_pd_dic, scaler_dic, feature_cols):\n",
    "    predict_res_final_2 = []\n",
    "    for date in next_date_range:\n",
    "        for eid in etf_ids:\n",
    "            #do predict\n",
    "            pred_data = next_pd_dic[eid.strip()][feature_cols].values\n",
    "            pred_res = model2.predict(data=pred_data)\n",
    "#             print(eid.strip(), '-', date, ': ', pred_res[0], '-', \n",
    "#                   scaler_dic[eid.strip()+'-etf_close'].inverse_transform(pred_res[0])[0][0])\n",
    "\n",
    "            etf_dt = etf_dic[eid.strip()]\n",
    "            prev_dt = etf_dt.loc[(etf_dt[\"etf_date\"] < date) ].iloc[-1]\n",
    "    #         print('id of prev_dt: ', eid, ', date: ' ,prev_dt[\"etf_date\"])\n",
    "            prev_close = prev_dt[\"etf_close\"]\n",
    "            scaler = scaler_dic['etf_close']\n",
    "            prediction = scaler.inverse_transform(pred_res[0])[0][0]\n",
    "            prediction = max_7percent(prev_close, prediction)\n",
    "            #取出所有預測結果作合併，以進行後續成績計算\n",
    "            predict_res_final_2.append({\n",
    "                'etf_id': eid, 'etf_date': date,\n",
    "                'prediction': prediction, 'pred_ud': judge_up_down_pred_native(prediction, prev_close)\n",
    "            })\n",
    "\n",
    "            if date != next_date_range[-1]:\n",
    "                #作出新的next_pd\n",
    "                new_dic = {'etf_id': [eid], 'etf_date': [date], 'etf_name': [''], \n",
    "                           'etf_open': [0.0],  'etf_high': [0.0], 'etf_low': [0.0], 'etf_close': [prediction], 'etf_count': [0.0]}\n",
    "                new_pd = pd.DataFrame(data=new_dic)[['etf_id','etf_date','etf_name', 'etf_open', 'etf_high', \n",
    "                                                          'etf_low', 'etf_close', 'etf_count']]\n",
    "                #print(new_pd)\n",
    "                etf_dt=etf_dt.append(new_pd, ignore_index=True)\n",
    "                next_date = next_date_range[next_date_range.index(date)+1]\n",
    "                etf_pd2, next_pd2, _ = create_feature(etf_dt, next_date, scaler_dic)\n",
    "                etf_dic.update({eid.strip(): etf_dt})\n",
    "                next_pd_dic.update({eid.strip(): next_pd2})\n",
    "    return predict_res_final_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for subset in itertools.combinations(feature_cols, 1):\n",
    "#     print(list(subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dic = {\n",
    "    'bias_ma,ema30,macdsignal1,rocr': '2',\n",
    "    'bias_3_6,ema20,ema30,macdsignal1': '1',\n",
    "    'bias_3_6,ema20,macdsignal1,rocr': '2',\n",
    "    'bias_ma,ema20,macdsignal1,ppo': '1',\n",
    "    'bias_ma,ema30,macdsignal1,nbias3': '2',\n",
    "#     'bias_ma,ema30,macdsignal1,rsi': '2',\n",
    "#     'bias_ma,ema30,macdhist1,macdsignal1': '1',\n",
    "    'bias_3_6,ema20,ema30,macd1': '2',\n",
    "    'bias_3_6,ema20,macd1,nbias6': '2',\n",
    "    'bias_ma,ema30,macdhist1,ppo': '2',\n",
    "    'bias_3_6,ema20,ema30,ema3_slope': '2',\n",
    "    'bias_3_6,ema20,nbias6,ppo': '2',\n",
    "#     'bias_3_6,ema20,ema30,nbias6': '2',\n",
    "#     'bias_ma,ema30,macd1,ppo': '2'\n",
    "}\n",
    "\n",
    "feature_dic = {\n",
    "    'ema5,k': '2',\n",
    "    'ema5_slope,k': '1',\n",
    "    'ema20,nbias6': '2',\n",
    "    'dif,ema3': '1',\n",
    "    'ema20,rocr': '2',\n",
    "    'ema5,rsi': '2',\n",
    "    'ema3,k': '1'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ema5,k : final2= 14.675701415916707\n",
      "ema5_slope,k : final1= 14.612784243001942\n",
      "ema20,nbias6 : final2= 14.567468270205813\n",
      "dif,ema3 : final1= 14.530728022265707\n",
      "ema20,rocr : final2= 14.522197100150134\n",
      "ema5,rsi : final2= 14.429681824244488\n",
      "ema3,k : final1= 14.414544119225145\n"
     ]
    }
   ],
   "source": [
    "type_list = []\n",
    "feature_list = []\n",
    "acc_list = []\n",
    "rmse_list = []\n",
    "escore_list = []\n",
    "all_pred_list = []\n",
    "\n",
    "# for features in itertools.combinations(feature_cols, feature_num):\n",
    "for feature_str in feature_dic.keys():    \n",
    "    f_type = feature_dic[feature_str]\n",
    "    feature_cols = list(feature_str.split(','))\n",
    "    model = trainModel(train_dt, feature_cols, f_type)\n",
    "    #recover to re-run\n",
    "    for eid in etf_ids:\n",
    "        etf_dt = etf_dic[eid.strip()]\n",
    "        etf_dt = etf_dt.loc[~etf_dt[\"etf_date\"].isin(next_date_range)]\n",
    "        etf_dic.update({eid.strip(): etf_dt})\n",
    "        etf_pd, next_pd, _ = create_feature(etf_dic[eid.strip()], predict_start_date, scaler_dic)\n",
    "#         print(eid.strip(), ': ', etf_pd.shape)\n",
    "        etf_pd_dic.update({eid.strip() : etf_pd})\n",
    "        next_pd_dic.update({eid.strip() : next_pd})\n",
    "    if f_type == '1':\n",
    "        #evaluate close_dif\n",
    "        predict_res_pd_1 = doPredict1(next_date_range, model, etf_dic, next_pd_dic, scaler_dic, feature_cols)\n",
    "        all_pred_list = all_pred_list + predict_res_pd_1\n",
    "        if DoEval:\n",
    "            rmse1, acc1, e_score1 = doEvaluate(predict_res_pd_1, tetf_pd_eval)\n",
    "            type_list.append(\"1\")\n",
    "            feature_list.append(feature_str)\n",
    "            acc_list.append(acc1)\n",
    "            rmse_list.append(rmse1)\n",
    "            escore_list.append(e_score1)\n",
    "        if DoEval:\n",
    "            print(feature_str, ': final1=', e_score1)\n",
    "    else:\n",
    "        #evaluate etf_close\n",
    "        predict_res_pd_2 = doPredict2(next_date_range, model, etf_dic, next_pd_dic, scaler_dic, feature_cols)\n",
    "        all_pred_list = all_pred_list + predict_res_pd_2\n",
    "        if DoEval:\n",
    "            rmse2, acc2, e_score2 = doEvaluate(predict_res_pd_2, tetf_pd_eval)\n",
    "            type_list.append(\"2\")\n",
    "            feature_list.append(feature_str)\n",
    "            acc_list.append(acc2)\n",
    "            rmse_list.append(rmse2)\n",
    "            escore_list.append(e_score2)\n",
    "        if DoEval:\n",
    "            print(feature_str, ': final2=', e_score2)\n",
    "\n",
    "if DoEval:\n",
    "    score_dic = {\n",
    "        'feature_type' : type_list, 'feature': feature_list, 'accuracy': acc_list, 'rmse': rmse_list,\n",
    "        'final_score': escore_list\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# etf_dic['0050']\n",
    "# score_dic['feature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_pd = pd.DataFrame(data=score_dic)[['feature_type', 'feature', 'accuracy', 'rmse', 'final_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_type</th>\n",
       "      <th>feature</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>rmse</th>\n",
       "      <th>final_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>ema5,k</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.443365</td>\n",
       "      <td>14.675701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ema5_slope,k</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.257648</td>\n",
       "      <td>14.612784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ema20,nbias6</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.638620</td>\n",
       "      <td>14.567468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>dif,ema3</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.286638</td>\n",
       "      <td>14.530728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>ema20,rocr</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.516883</td>\n",
       "      <td>14.522197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>ema5,rsi</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.419822</td>\n",
       "      <td>14.429682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>ema3,k</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.283337</td>\n",
       "      <td>14.414544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  feature_type       feature  accuracy      rmse  final_score\n",
       "0            2        ema5,k  0.622222  0.443365    14.675701\n",
       "1            1  ema5_slope,k  0.622222  0.257648    14.612784\n",
       "2            2  ema20,nbias6  0.600000  0.638620    14.567468\n",
       "3            1      dif,ema3  0.611111  0.286638    14.530728\n",
       "4            2    ema20,rocr  0.611111  0.516883    14.522197\n",
       "5            2      ema5,rsi  0.577778  0.419822    14.429682\n",
       "6            1        ema3,k  0.600000  0.283337    14.414544"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_pd.sort_values(by=['final_score'], ascending=[0]).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_pred_pd = pd.DataFrame(all_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20180528  final: rmse= 0.20114830914596776 , accuracy= 0.7222222222222222 etf_score= 15.824572997546214\n"
     ]
    }
   ],
   "source": [
    "def judge_ud(ud_list):\n",
    "    num_1 = 0\n",
    "    num_0 = 0\n",
    "    num_2 = 0\n",
    "    for ud in ud_list:\n",
    "        if ud == 1.0:\n",
    "            num_1 += 1\n",
    "        elif ud == 2.0:\n",
    "            num_2 += 1\n",
    "        else:\n",
    "            num_0 += 1\n",
    "    if num_2 >= num_1 and num_2 >= num_0:\n",
    "        return 2.0\n",
    "    if num_1 >= num_2 and num_1 >= num_0:\n",
    "        return 1.0\n",
    "    return 0.0\n",
    "    \n",
    "predict_res_final = []\n",
    "for date in next_date_range:\n",
    "    for eid in etf_ids:\n",
    "        pred_pd = all_pred_pd.loc[(all_pred_pd['etf_id'] == eid) & (all_pred_pd['etf_date'] == date)]\n",
    "        ud_list = pred_pd['pred_ud'].tolist()\n",
    "        ud = judge_ud(ud_list)\n",
    "        close_list = pred_pd[\"prediction\"].tolist()\n",
    "        close = avg_list(close_list)\n",
    "        predict_res_final.append({\n",
    "            'etf_id': eid, 'etf_date': date,\n",
    "            'prediction': close, 'pred_ud': ud\n",
    "        })\n",
    "\n",
    "if DoEval:\n",
    "    rmse, acc, e_score = doEvaluate(predict_res_final, tetf_pd_eval)\n",
    "    print(predict_start_date, ' final: rmse=', rmse, ', accuracy=', acc, 'etf_score=', e_score)\n",
    "# 20180604 final: rmse= 0.4193046180662565 , accuracy= 0.5888888888888889 etf_score= 14.05098652556833\n",
    "# 20180528  final: rmse= 0.3079296444208076 , accuracy= 0.45555555555555555 etf_score= 13.240249626243987\n",
    "# 20180521  final: rmse= 0.6377778669673745 , accuracy= 0.43333333333333335 etf_score= 13.15790992705023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ETFid</th>\n",
       "      <th>Mon_ud</th>\n",
       "      <th>Mon_cprice</th>\n",
       "      <th>Tue_ud</th>\n",
       "      <th>Tue_cprice</th>\n",
       "      <th>Wed_ud</th>\n",
       "      <th>Wed_cprice</th>\n",
       "      <th>Thu_ud</th>\n",
       "      <th>Thu_cprice</th>\n",
       "      <th>Fri_ud</th>\n",
       "      <th>Fri_cprice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0050</td>\n",
       "      <td>1</td>\n",
       "      <td>81.886805</td>\n",
       "      <td>1</td>\n",
       "      <td>82.031515</td>\n",
       "      <td>-1</td>\n",
       "      <td>81.790991</td>\n",
       "      <td>1</td>\n",
       "      <td>81.832052</td>\n",
       "      <td>1</td>\n",
       "      <td>81.980317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0051</td>\n",
       "      <td>1</td>\n",
       "      <td>33.600014</td>\n",
       "      <td>-1</td>\n",
       "      <td>33.249032</td>\n",
       "      <td>-1</td>\n",
       "      <td>33.398497</td>\n",
       "      <td>1</td>\n",
       "      <td>33.628114</td>\n",
       "      <td>1</td>\n",
       "      <td>33.767901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0052</td>\n",
       "      <td>1</td>\n",
       "      <td>52.517097</td>\n",
       "      <td>1</td>\n",
       "      <td>52.612934</td>\n",
       "      <td>1</td>\n",
       "      <td>52.677188</td>\n",
       "      <td>1</td>\n",
       "      <td>53.115333</td>\n",
       "      <td>1</td>\n",
       "      <td>53.198699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0053</td>\n",
       "      <td>1</td>\n",
       "      <td>35.827111</td>\n",
       "      <td>-1</td>\n",
       "      <td>35.910573</td>\n",
       "      <td>-1</td>\n",
       "      <td>35.653103</td>\n",
       "      <td>1</td>\n",
       "      <td>35.655833</td>\n",
       "      <td>1</td>\n",
       "      <td>35.775665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0054</td>\n",
       "      <td>-1</td>\n",
       "      <td>24.154740</td>\n",
       "      <td>-1</td>\n",
       "      <td>24.202935</td>\n",
       "      <td>-1</td>\n",
       "      <td>24.103771</td>\n",
       "      <td>-1</td>\n",
       "      <td>24.096877</td>\n",
       "      <td>-1</td>\n",
       "      <td>24.020570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0055</td>\n",
       "      <td>1</td>\n",
       "      <td>17.643941</td>\n",
       "      <td>1</td>\n",
       "      <td>17.722979</td>\n",
       "      <td>-1</td>\n",
       "      <td>17.665627</td>\n",
       "      <td>1</td>\n",
       "      <td>17.680132</td>\n",
       "      <td>1</td>\n",
       "      <td>17.688898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0056</td>\n",
       "      <td>1</td>\n",
       "      <td>26.321232</td>\n",
       "      <td>-1</td>\n",
       "      <td>26.244326</td>\n",
       "      <td>1</td>\n",
       "      <td>26.324357</td>\n",
       "      <td>1</td>\n",
       "      <td>26.309941</td>\n",
       "      <td>1</td>\n",
       "      <td>26.406610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0057</td>\n",
       "      <td>1</td>\n",
       "      <td>50.539566</td>\n",
       "      <td>-1</td>\n",
       "      <td>50.125170</td>\n",
       "      <td>-1</td>\n",
       "      <td>49.935811</td>\n",
       "      <td>-1</td>\n",
       "      <td>49.851934</td>\n",
       "      <td>1</td>\n",
       "      <td>49.909630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0058</td>\n",
       "      <td>1</td>\n",
       "      <td>47.249010</td>\n",
       "      <td>-1</td>\n",
       "      <td>47.043276</td>\n",
       "      <td>-1</td>\n",
       "      <td>46.958909</td>\n",
       "      <td>1</td>\n",
       "      <td>47.039399</td>\n",
       "      <td>-1</td>\n",
       "      <td>47.045368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0059</td>\n",
       "      <td>-1</td>\n",
       "      <td>42.764189</td>\n",
       "      <td>1</td>\n",
       "      <td>42.744997</td>\n",
       "      <td>-1</td>\n",
       "      <td>42.475038</td>\n",
       "      <td>1</td>\n",
       "      <td>42.506621</td>\n",
       "      <td>1</td>\n",
       "      <td>42.693560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>006201</td>\n",
       "      <td>-1</td>\n",
       "      <td>14.419167</td>\n",
       "      <td>-1</td>\n",
       "      <td>14.334299</td>\n",
       "      <td>1</td>\n",
       "      <td>14.376974</td>\n",
       "      <td>1</td>\n",
       "      <td>14.364503</td>\n",
       "      <td>-1</td>\n",
       "      <td>14.364523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>006203</td>\n",
       "      <td>1</td>\n",
       "      <td>38.237001</td>\n",
       "      <td>1</td>\n",
       "      <td>38.249007</td>\n",
       "      <td>-1</td>\n",
       "      <td>38.106841</td>\n",
       "      <td>-1</td>\n",
       "      <td>37.901359</td>\n",
       "      <td>1</td>\n",
       "      <td>38.088709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>006204</td>\n",
       "      <td>1</td>\n",
       "      <td>54.597176</td>\n",
       "      <td>1</td>\n",
       "      <td>54.486613</td>\n",
       "      <td>-1</td>\n",
       "      <td>54.270149</td>\n",
       "      <td>-1</td>\n",
       "      <td>54.185478</td>\n",
       "      <td>1</td>\n",
       "      <td>54.274445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>006208</td>\n",
       "      <td>-1</td>\n",
       "      <td>47.848933</td>\n",
       "      <td>-1</td>\n",
       "      <td>47.682297</td>\n",
       "      <td>-1</td>\n",
       "      <td>47.576275</td>\n",
       "      <td>-1</td>\n",
       "      <td>47.488036</td>\n",
       "      <td>1</td>\n",
       "      <td>47.573291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00690</td>\n",
       "      <td>-1</td>\n",
       "      <td>22.072314</td>\n",
       "      <td>-1</td>\n",
       "      <td>21.864136</td>\n",
       "      <td>-1</td>\n",
       "      <td>21.849649</td>\n",
       "      <td>-1</td>\n",
       "      <td>21.828938</td>\n",
       "      <td>1</td>\n",
       "      <td>21.798179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00692</td>\n",
       "      <td>1</td>\n",
       "      <td>21.200473</td>\n",
       "      <td>-1</td>\n",
       "      <td>21.181627</td>\n",
       "      <td>1</td>\n",
       "      <td>21.283840</td>\n",
       "      <td>-1</td>\n",
       "      <td>21.360397</td>\n",
       "      <td>1</td>\n",
       "      <td>21.425940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00701</td>\n",
       "      <td>-1</td>\n",
       "      <td>21.126243</td>\n",
       "      <td>-1</td>\n",
       "      <td>21.105254</td>\n",
       "      <td>-1</td>\n",
       "      <td>21.082697</td>\n",
       "      <td>1</td>\n",
       "      <td>21.159623</td>\n",
       "      <td>1</td>\n",
       "      <td>21.242905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00713</td>\n",
       "      <td>1</td>\n",
       "      <td>31.069156</td>\n",
       "      <td>-1</td>\n",
       "      <td>30.948022</td>\n",
       "      <td>-1</td>\n",
       "      <td>30.959702</td>\n",
       "      <td>1</td>\n",
       "      <td>30.963200</td>\n",
       "      <td>1</td>\n",
       "      <td>31.004785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ETFid  Mon_ud  Mon_cprice  Tue_ud  Tue_cprice  Wed_ud  Wed_cprice  \\\n",
       "0   0050          1   81.886805       1   82.031515      -1   81.790991   \n",
       "1   0051          1   33.600014      -1   33.249032      -1   33.398497   \n",
       "2   0052          1   52.517097       1   52.612934       1   52.677188   \n",
       "3   0053          1   35.827111      -1   35.910573      -1   35.653103   \n",
       "4   0054         -1   24.154740      -1   24.202935      -1   24.103771   \n",
       "5   0055          1   17.643941       1   17.722979      -1   17.665627   \n",
       "6   0056          1   26.321232      -1   26.244326       1   26.324357   \n",
       "7   0057          1   50.539566      -1   50.125170      -1   49.935811   \n",
       "8   0058          1   47.249010      -1   47.043276      -1   46.958909   \n",
       "9   0059         -1   42.764189       1   42.744997      -1   42.475038   \n",
       "10  006201       -1   14.419167      -1   14.334299       1   14.376974   \n",
       "11  006203        1   38.237001       1   38.249007      -1   38.106841   \n",
       "12  006204        1   54.597176       1   54.486613      -1   54.270149   \n",
       "13  006208       -1   47.848933      -1   47.682297      -1   47.576275   \n",
       "14  00690        -1   22.072314      -1   21.864136      -1   21.849649   \n",
       "15  00692         1   21.200473      -1   21.181627       1   21.283840   \n",
       "16  00701        -1   21.126243      -1   21.105254      -1   21.082697   \n",
       "17  00713         1   31.069156      -1   30.948022      -1   30.959702   \n",
       "\n",
       "    Thu_ud  Thu_cprice  Fri_ud  Fri_cprice  \n",
       "0        1   81.832052       1   81.980317  \n",
       "1        1   33.628114       1   33.767901  \n",
       "2        1   53.115333       1   53.198699  \n",
       "3        1   35.655833       1   35.775665  \n",
       "4       -1   24.096877      -1   24.020570  \n",
       "5        1   17.680132       1   17.688898  \n",
       "6        1   26.309941       1   26.406610  \n",
       "7       -1   49.851934       1   49.909630  \n",
       "8        1   47.039399      -1   47.045368  \n",
       "9        1   42.506621       1   42.693560  \n",
       "10       1   14.364503      -1   14.364523  \n",
       "11      -1   37.901359       1   38.088709  \n",
       "12      -1   54.185478       1   54.274445  \n",
       "13      -1   47.488036       1   47.573291  \n",
       "14      -1   21.828938       1   21.798179  \n",
       "15      -1   21.360397       1   21.425940  \n",
       "16       1   21.159623       1   21.242905  \n",
       "17       1   30.963200       1   31.004785  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_res = spark.createDataFrame(pd.DataFrame(predict_res_final)).orderBy(\"etf_id\", \"etf_date\").collect()\n",
    "#export to pandas\n",
    "etf_ids = []\n",
    "mon_ud = []\n",
    "mon_price = [] \n",
    "tue_ud = []\n",
    "tue_price = []\n",
    "wed_ud = []\n",
    "wed_price = []\n",
    "thu_ud = []\n",
    "thu_price = []\n",
    "fri_ud = []\n",
    "fri_price = []\n",
    "\n",
    "def encode_ud(oper_ud):\n",
    "    if oper_ud == 0.0:\n",
    "        return 0\n",
    "    elif oper_ud == 1.0:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "for row in final_res:\n",
    "    etf_id = row[\"etf_id\"]\n",
    "    if etf_id not in etf_ids:\n",
    "        etf_ids.append(etf_id)\n",
    "    etf_date = row[\"etf_date\"]\n",
    "    eidx = next_date_range.index(row[\"etf_date\"])\n",
    "    if eidx == 0:\n",
    "        mon_ud.append(encode_ud(row[\"pred_ud\"]))\n",
    "        mon_price.append(row[\"prediction\"])\n",
    "    elif eidx == 1:\n",
    "        tue_ud.append(encode_ud(row[\"pred_ud\"]))\n",
    "        tue_price.append(row[\"prediction\"])\n",
    "    elif eidx == 2:\n",
    "        wed_ud.append(encode_ud(row[\"pred_ud\"]))\n",
    "        wed_price.append(row[\"prediction\"])\n",
    "    elif eidx == 3:\n",
    "        thu_ud.append(encode_ud(row[\"pred_ud\"]))\n",
    "        thu_price.append(row[\"prediction\"])\n",
    "    elif eidx == 4:\n",
    "        fri_ud.append(encode_ud(row[\"pred_ud\"]))\n",
    "        fri_price.append(row[\"prediction\"])\n",
    "        \n",
    "if len(mon_ud) == 0:\n",
    "    mon_ud = list(0 for i in range(0,len(etf_ids)))\n",
    "    mon_price = list(0.0 for i in range(0,len(etf_ids)))\n",
    "if len(tue_ud) == 0:\n",
    "    tue_ud = list(0 for i in range(0,len(etf_ids)))\n",
    "    tue_price = list(0.0 for i in range(0,len(etf_ids)))\n",
    "if len(wed_ud) == 0:\n",
    "    wed_ud = list(0 for i in range(0,len(etf_ids)))\n",
    "    wed_price = list(0.0 for i in range(0,len(etf_ids)))\n",
    "if len(thu_ud) == 0:\n",
    "    thu_ud = list(0 for i in range(0,len(etf_ids)))\n",
    "    thu_price = list(0.0 for i in range(0,len(etf_ids)))\n",
    "if len(fri_ud) == 0:\n",
    "    fri_ud = list(0 for i in range(0,len(etf_ids)))\n",
    "    fri_price = list(0.0 for i in range(0,len(etf_ids)))\n",
    "    \n",
    "dic = {\"ETFid\": etf_ids, \n",
    "       \"Mon_ud\": mon_ud, \"Mon_cprice\": mon_price,\n",
    "       \"Tue_ud\": tue_ud, \"Tue_cprice\": tue_price,\n",
    "       \"Wed_ud\": wed_ud, \"Wed_cprice\": wed_price,\n",
    "       \"Thu_ud\": thu_ud, \"Thu_cprice\": thu_price,\n",
    "       \"Fri_ud\": fri_ud, \"Fri_cprice\": fri_price\n",
    "      }\n",
    "final_df = pd.DataFrame(data=dic)[['ETFid','Mon_ud','Mon_cprice','Tue_ud','Tue_cprice',\n",
    "                                  'Wed_ud','Wed_cprice','Thu_ud','Thu_cprice',\n",
    "                                  'Fri_ud','Fri_cprice']]\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(Path.replace(\"file:\",\"\") + \"/etf_price_pred_talib_eval_mac.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, udf, lag, rank, lit\n",
    "from pyspark.sql.window import Window\n",
    "import talib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "global DoEval #是否進行模型評估\n",
    "\n",
    "# DoEval = False\n",
    "# next_date_range = [\"20180611\", \"20180612\", \"20180613\", \"20180614\", \"20180615\"] #設定預測區間\n",
    "next_date_range = [\"20180604\", \"20180605\", \"20180606\", \"20180607\", \"20180608\"] #設定預測區間\n",
    "# next_date_range = [\"20180528\", \"20180529\", \"20180530\", \"20180531\", \"20180601\"] #設定預測區間\n",
    "# next_date_range = [\"20180521\", \"20180522\", \"20180523\", \"20180524\", \"20180525\"] #設定預測區間\n",
    "DoEval = True\n",
    "# next_date_range = [\"20180514\", \"20180515\", \"20180516\", \"20180517\", \"20180518\"] #設定預測區間\n",
    "ignore_dates = [\"\"]#設定排除日(如端午節)\n",
    "# next_date_range = [\"20180507\", \"20180508\", \"20180509\", \"20180510\", \"20180511\"] #設定預測區間\n",
    "predict_start_date = next_date_range[0]\n",
    "\n",
    "if sc.master[0:5]==\"local\":\n",
    "#     Path = \"file:/c:/D Drive/work/bigData/pySpark/TBrain_Round2_DataSet_20180601\"\n",
    "    #Path = \"file:/Users/yungchuanlee/Documents/learn/AI競賽/ETF預測/TBrain_Round2_DataSet_20180511\"\n",
    "    Path = \"file:/home/hduser/app/bigdata/competition/etf/TBrain_Round2_DataSet_20180518\"\n",
    "else:\n",
    "    Path = \"hdfs://master:9000/user/hduser\"\n",
    "Path = \"file:/home/hduser/app/bigdata/competition/etf/TBrain_Round2_DataSet_20180608\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define alias of columns\n",
    "col_alias_etf= {\"代碼\":\"etf_id\", \"日期\": \"etf_date\", \"中文簡稱\": \"etf_name\", \"開盤價(元)\":\"etf_open\", \n",
    "            \"最高價(元)\":\"etf_high\", \"最低價(元)\":\"etf_low\", \"收盤價(元)\":\"etf_close\", \"成交張數(張)\":\"etf_count\"}\n",
    "col_alias_stock= {\"代碼\":\"stock_id\", \"日期\": \"stock_date\", \"中文簡稱\": \"stock_name\", \"開盤價(元)\":\"stock_open\", \n",
    "            \"最高價(元)\":\"stock_high\", \"最低價(元)\":\"stock_low\", \"收盤價(元)\":\"stock_close\", \"成交張數(張)\":\"stock_count\"}#udf\n",
    "def to_double(str_val):\n",
    "    return float(str_val.replace(\",\",\"\"))\n",
    "to_double=udf(to_double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def function to read data (因檔案格式都相同)\n",
    "def read_data(file_name, col_alias):\n",
    "    str_cols = [\"代碼\",\"日期\", \"中文簡稱\"]\n",
    "    raw_data = spark.read.option(\"encoding\", \"Big5\").csv(Path + \"/\" + file_name, header=True, sep=\",\")\n",
    "    print(\"Total \" + file_name + \" count: \" + str(raw_data.count()))\n",
    "    #rename cols and correct type \n",
    "    num_cols = [col_name for col_name in raw_data.columns if col_name not in str_cols]\n",
    "    final_data=raw_data.select( [col(str_col_name).alias(col_alias[str_col_name]) for str_col_name in str_cols] + \n",
    "                                  [to_double(col(num_col_name)).cast(\"double\").alias(col_alias[num_col_name]) for num_col_name in num_cols] )\n",
    "    final_data.printSchema()\n",
    "    final_data.show(5)\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting import tetfp.csv(台灣18檔ETF股價資料)...\n",
      "Total tetfp.csv count: 19575\n",
      "root\n",
      " |-- etf_id: string (nullable = true)\n",
      " |-- etf_date: string (nullable = true)\n",
      " |-- etf_name: string (nullable = true)\n",
      " |-- etf_open: double (nullable = true)\n",
      " |-- etf_high: double (nullable = true)\n",
      " |-- etf_low: double (nullable = true)\n",
      " |-- etf_close: double (nullable = true)\n",
      " |-- etf_count: double (nullable = true)\n",
      "\n",
      "+-------+--------+----------------+--------+--------+-------+---------+---------+\n",
      "| etf_id|etf_date|        etf_name|etf_open|etf_high|etf_low|etf_close|etf_count|\n",
      "+-------+--------+----------------+--------+--------+-------+---------+---------+\n",
      "|0050   |20130102|元大台灣50          |    54.0|   54.65|   53.9|     54.4|  16487.0|\n",
      "|0050   |20130103|元大台灣50          |    54.9|   55.05|  54.65|    54.85|  29020.0|\n",
      "|0050   |20130104|元大台灣50          |   54.85|   54.85|   54.4|     54.5|   9837.0|\n",
      "|0050   |20130107|元大台灣50          |   54.55|   54.55|   53.9|    54.25|   8910.0|\n",
      "|0050   |20130108|元大台灣50          |    54.0|    54.2|  53.65|     53.9|  12507.0|\n",
      "+-------+--------+----------------+--------+--------+-------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"starting import tetfp.csv(台灣18檔ETF股價資料)...\")\n",
    "tetfp_dt=read_data(\"tetfp.csv\", col_alias_etf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0050   ': 0,\n",
       " '0051   ': 1,\n",
       " '0052   ': 2,\n",
       " '0053   ': 3,\n",
       " '0054   ': 4,\n",
       " '0055   ': 5,\n",
       " '0056   ': 6,\n",
       " '0057   ': 7,\n",
       " '0058   ': 8,\n",
       " '0059   ': 9,\n",
       " '006201 ': 10,\n",
       " '006203 ': 11,\n",
       " '006204 ': 12,\n",
       " '006208 ': 13,\n",
       " '00690  ': 14,\n",
       " '00692  ': 15,\n",
       " '00701  ': 16,\n",
       " '00713  ': 17}"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#取出etf的distinct id\n",
    "etf_ids = []\n",
    "etf_idx_dic = {}\n",
    "etf_idx = 0\n",
    "for row in tetfp_dt.select(\"etf_id\").distinct().orderBy(\"etf_id\").collect():\n",
    "    etf_ids.append(row[\"etf_id\"])\n",
    "    etf_idx_dic.update({row[\"etf_id\"]: etf_idx})\n",
    "    etf_idx += 1\n",
    "etf_idx_dic\n",
    "# etf_ids = ['0050   ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "etf_dic = {}\n",
    "for etfid in etf_ids:\n",
    "    export_dt = tetfp_dt.filter(\"etf_id='\" +etfid+ \"' and etf_date < '\" + predict_start_date + \"'\") \\\n",
    "        .orderBy(\"etf_id\", \"etf_date\", ascending=True)\n",
    "    export_pd = export_dt.toPandas()\n",
    "    etf_dic.update({etfid.strip(): export_pd})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions of TA lib\n",
    "#print(talib.get_functions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1327"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etf_dic[\"0050\"][\"etf_close\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def avg_list(p_list):\n",
    "    #計算數字list的平均值\n",
    "    return sum(p_list)/len(p_list)\n",
    "\n",
    "def get_feature_pre(curr_list):\n",
    "    #在feature前加上一個Nan後回傳, 第二個值則回傳原本的最後一個值作為下次的feature\n",
    "    if type(curr_list) is pd.Series:\n",
    "        rtn_list = curr_list.tolist()\n",
    "    else:\n",
    "        rtn_list = curr_list\n",
    "    rtn_list = [None] + rtn_list\n",
    "    return (rtn_list[:-1], [rtn_list[-1]])\n",
    "\n",
    "def checkNan(num):\n",
    "    if num == None:\n",
    "        return True\n",
    "    elif math.isnan(num):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def calculate_n_bias(close_price, ma):\n",
    "#     (close price - MA)/MA\n",
    "    if checkNan(close_price) or checkNan(ma):\n",
    "        return None\n",
    "    else:\n",
    "        return (close_price - ma)/ma\n",
    "    \n",
    "def calculate_n_bias_list(close_ser, ma_list):\n",
    "    #calculate n bias (20 days)\n",
    "    bias_list = []\n",
    "    for cprice, ma in zip(close_ser.tolist(), ma_list):\n",
    "        bias_list.append(calculate_n_bias(cprice, ma))\n",
    "    return bias_list\n",
    "\n",
    "def calculate_bias_ma_list(ma3_list, ma6_list):\n",
    "    #calculate bias_ma = (ma3-ma6)/ma6\n",
    "    bias_list = []\n",
    "    for ma3, ma6 in zip(ma3_list, ma6_list):\n",
    "        bias_list.append(calculate_n_bias(ma3, ma6))\n",
    "    return bias_list\n",
    "\n",
    "def calculate_bias_3_6_list(bias3_list, bias6_list):\n",
    "    #bias_3_6 = bias3 - bias6\n",
    "    bias_list = []\n",
    "    for bias3, bias6 in zip(bias3_list, bias6_list):\n",
    "        if checkNan(bias3) or checkNan(bias6):\n",
    "            bias_list.append(None)\n",
    "        else:\n",
    "            bias_list.append(bias3 - bias6)\n",
    "    return bias_list\n",
    "\n",
    "def calculate_dif(ema12_list, ema26_list):\n",
    "    #差離值DIF = 12日EMA - 26日EMA \n",
    "    dif_list = []\n",
    "    for ema12, ema26 in zip(ema12_list, ema26_list):\n",
    "        if checkNan(ema12) or checkNan(ema26):\n",
    "            dif_list.append(None)\n",
    "        else:\n",
    "            dif_list.append(ema12 - ema26)\n",
    "    return dif_list\n",
    "def calculate_slope(cur_list, prev_list):\n",
    "    slope_list = []\n",
    "    for cur, prev in zip(cur_list, prev_list):\n",
    "        if checkNan(cur) or checkNan(prev):\n",
    "            slope_list.append(None)\n",
    "        else:\n",
    "            slope_list.append((cur-prev)/prev)\n",
    "    return slope_list\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import itertools\n",
    "def create_feature(etf_pd, next_date, scaler_dic):\n",
    "    close_ser = etf_pd[\"etf_close\"]\n",
    "    #EMA\n",
    "    ema3 = talib.EMA(close_ser,timeperiod=3)\n",
    "    ema3_prev, _ = get_feature_pre(ema3)\n",
    "    ema3_slope = calculate_slope(ema3, ema3_prev)\n",
    "    ema5 = talib.EMA(close_ser,timeperiod=5)\n",
    "    ema5_prev, _ = get_feature_pre(ema5)\n",
    "    ema5_slope = calculate_slope(ema5, ema5_prev)\n",
    "    ema10 = talib.EMA(close_ser,timeperiod=10)\n",
    "    ema10_prev, _ = get_feature_pre(ema10)\n",
    "    ema10_slope = calculate_slope(ema10, ema10_prev)\n",
    "    ema20 = talib.EMA(close_ser,timeperiod=20)\n",
    "    ema30 = talib.EMA(close_ser,timeperiod=30)\n",
    "    \n",
    "    #make diff\n",
    "    close_ser_prev, _ = get_feature_pre(close_ser)\n",
    "    close_diff = calculate_dif(close_ser.tolist(), close_ser_prev)\n",
    "    \n",
    "    close_diff_2, _ = get_feature_pre(close_diff)\n",
    "    close_diff_3, _ = get_feature_pre(close_diff_2)\n",
    "    #BIAS\n",
    "    #nBIAS -3, 6, 20 => (close price - MA)/MA   ,Paper 建議用20日MA\n",
    "    ma3 = talib.MA(np.array(close_ser), timeperiod=3)\n",
    "    ma6 = talib.MA(np.array(close_ser), timeperiod=6)\n",
    "    ma20 = talib.MA(np.array(close_ser), timeperiod=20)\n",
    "    nbias3 = calculate_n_bias_list(close_ser, ma3)\n",
    "    nbias6 = calculate_n_bias_list(close_ser, ma6)\n",
    "    nbias20 = calculate_n_bias_list(close_ser, ma20)\n",
    "    \n",
    "    #BIAS_ma = (ma3-ma6)/ma6\n",
    "    bias_ma = calculate_bias_ma_list(ma3, ma6)\n",
    "    \n",
    "    #bias_3_6 = bias3 - bias6\n",
    "    bias_3_6 = calculate_bias_3_6_list(nbias3, nbias6)\n",
    "    \n",
    "    #KD --> only use STOCHRSI\n",
    "    k, d = talib.STOCHRSI(close_ser, timeperiod=9, fastk_period=3, fastd_period=3, fastd_matype=0)\n",
    "    \n",
    "    #差離值DIF = 12日EMA - 26日EMA \n",
    "    ema12 = talib.EMA(close_ser,timeperiod=12)\n",
    "    ema26 = talib.EMA(close_ser,timeperiod=26)\n",
    "    dif = calculate_dif(ema12, ema26)\n",
    "    \n",
    "    #MACD\n",
    "#     macd1, macdsignal1, macdhist1 = talib.MACD(close_ser, fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "    macd1, macdsignal1, macdhist1 = talib.MACDEXT(close_ser, fastperiod=12, fastmatype=0, \n",
    "                                               slowperiod=26, slowmatype=0, signalperiod=9, signalmatype=0)\n",
    "#     macd1, macdsignal1, macdhist1 = talib.MACDFIX(close_ser, signalperiod=9)\n",
    "    #RSI\n",
    "    rsi = talib.RSI(close_ser, timeperiod=10)\n",
    "    \n",
    "    #CMO\n",
    "    cmo = talib.CMO(close_ser, timeperiod=14)\n",
    "    \n",
    "    #ROCR - Rate of change ratio: (price/prevPrice)\n",
    "    #rocr = talib.ROCR(close_ser, timeperiod=10)\n",
    "    #ROC - Rate of change : ((price/prevPrice)-1)*100\n",
    "    rocr = talib.ROC(close_ser, timeperiod=10)\n",
    "    \n",
    "    #PPO - Percentage Price Oscillator\n",
    "    ppo = talib.PPO(close_ser, fastperiod=12, slowperiod=26, matype=0)\n",
    "    #APO - Absolute Price Oscillator\n",
    "    apo = talib.APO(close_ser, fastperiod=12, slowperiod=26, matype=0)\n",
    "    \n",
    "    #MOM - Momentum\n",
    "    mom = talib.MOM(close_ser, timeperiod=10)\n",
    "    #TRIX - 1-day Rate-Of-Change (ROC) of a Triple Smooth EMA\n",
    "    trix = talib.TRIX(close_ser, timeperiod=30)\n",
    "    #作標準化\n",
    "    etfid = etf_pd[\"etf_id\"].tolist()[0]\n",
    "    tmp_pd = pd.DataFrame({\"etf_id\": etfid, \"etf_date\": etf_pd[\"etf_date\"], \"etf_close\": close_ser, \"close_dif\": close_diff,  \n",
    "#                            \"close_diff_1\": close_diff, \"close_diff_2\": close_diff_2, \"close_diff_3\": close_diff_3,\n",
    "                           \"ema3\":ema3, \"ema5\": ema5, \"ema10\": ema10, \"ema20\": ema20, \"ema30\" : ema30,\n",
    "                           \"ema3_slope\": ema3_slope, \"ema5_slope\": ema5_slope, \"ema10_slope\": ema10_slope,\n",
    "                          \"nbias3\": nbias3, \"nbias6\": nbias6, \"nbias20\": nbias20, \"bias_ma\": bias_ma,\n",
    "                          \"bias_3_6\": bias_3_6, \"k\": k, \"d\": d, \"dif\": dif,\n",
    "                          #\"macd\": macd, \"macdsignal\": macdsignal, \"macdhist\": macdhist,\n",
    "                          \"macd1\": macd1, \"macdsignal1\": macdsignal1, \"macdhist1\": macdhist1,\n",
    "                          \"rsi\": rsi, \"cmo\": cmo, \"rocr\": rocr, \"ppo\": ppo}).dropna(how='any')\n",
    "    tmp_pd = tmp_pd[tmp_pd['etf_date'] > '20140101']\n",
    "    #exclude outlier\n",
    "#     for col in [c for c in tmp_pd.columns.values if c not in [\"etf_id\", \"etf_date\"]]:\n",
    "#         tmp_pd = tmp_pd[np.abs(tmp_pd[col] - tmp_pd[col].mean()) <= 3.0*tmp_pd[col].std()] #只取3倍以內標準差資料\n",
    "    \n",
    "    std_dic = {}\n",
    "    for col in [c for c in tmp_pd.columns.values if c not in [\"etf_id\", \"etf_date\"]]:\n",
    "#         scaler_key = etfid.strip() + '-' + col\n",
    "        scaler_key = col\n",
    "        if scaler_key in scaler_dic:\n",
    "            scaler = scaler_dic[scaler_key]\n",
    "            merged = list(itertools.chain.from_iterable(scaler.transform(tmp_pd[[col]])))                \n",
    "#             merged = tmp_pd[col]\n",
    "        else:\n",
    "            scaler = MinMaxScaler()\n",
    "            merged = list(itertools.chain.from_iterable(scaler.fit_transform(tmp_pd[[col]])))\n",
    "            merged = tmp_pd[col]\n",
    "            #保留etf_close的scaler在後續使用\n",
    "            scaler_dic.update({scaler_key: scaler})\n",
    "        std_dic.update({col: merged})\n",
    "    \n",
    "    #作出目前的feature table及預測用的feature table\n",
    "    ema3, next_ema3 = get_feature_pre(std_dic[\"ema3\"])\n",
    "    ema3_slope, next_ema3_slope = get_feature_pre(std_dic[\"ema3_slope\"])\n",
    "    ema5, next_ema5 = get_feature_pre(std_dic[\"ema5\"])\n",
    "    ema5_slope, next_ema5_slope = get_feature_pre(std_dic[\"ema5_slope\"])\n",
    "    ema10, next_ema10 = get_feature_pre(std_dic[\"ema10\"])\n",
    "    ema10_slope, next_ema10_slope = get_feature_pre(std_dic[\"ema10_slope\"])\n",
    "    ema20, next_ema20 = get_feature_pre(std_dic[\"ema20\"])\n",
    "    ema30, next_ema30 = get_feature_pre(std_dic[\"ema30\"])\n",
    "    \n",
    "    nbias3, next_nbias3 = get_feature_pre(std_dic[\"nbias3\"])\n",
    "    nbias6, next_nbias6 = get_feature_pre(std_dic[\"nbias6\"])\n",
    "    nbias20, next_nbias20 = get_feature_pre(std_dic[\"nbias20\"])\n",
    "    \n",
    "    bias_ma, next_bias_ma = get_feature_pre(std_dic[\"bias_ma\"])\n",
    "    bias_3_6, next_bias_3_6 = get_feature_pre(std_dic[\"bias_3_6\"])\n",
    "    k, next_k = get_feature_pre(std_dic['k'])\n",
    "    d, next_d = get_feature_pre(std_dic['d'])\n",
    "    dif, next_dif = get_feature_pre(std_dic['dif'])\n",
    "    \n",
    "    #macd, next_macd = get_feature_pre(std_dic['macd'])\n",
    "    #macdsignal, next_macdsignal = get_feature_pre(std_dic['macdsignal'])\n",
    "    #macdhist, next_macdhist = get_feature_pre(std_dic['macdhist'])\n",
    "    macd1, next_macd1 = get_feature_pre(std_dic['macd1'])\n",
    "    macdsignal1, next_macdsignal1 = get_feature_pre(std_dic['macdsignal1'])\n",
    "    macdhist1, next_macdhist1 = get_feature_pre(std_dic['macdhist1'])\n",
    "                                               \n",
    "    rsi, next_rsi = get_feature_pre(std_dic['rsi'])\n",
    "    cmo, next_cmo = get_feature_pre(std_dic['cmo'])\n",
    "    rocr, next_rocr = get_feature_pre(std_dic['rocr'])\n",
    "    ppo, next_ppo = get_feature_pre(std_dic['ppo'])\n",
    "    \n",
    "#     close_diff_1, next_close_diff_1 = get_feature_pre(std_dic['close_diff_1'])\n",
    "#     close_diff_2, next_close_diff_2 = get_feature_pre(std_dic['close_diff_2'])\n",
    "#     close_diff_3, next_close_diff_3 = get_feature_pre(std_dic['close_diff_3'])\n",
    "    \n",
    "    #\"apo\": apo, \"mom\": mom, \"trix\": trix\n",
    "#     apo, next_apo = get_feature_pre(std_dic['apo'])\n",
    "#     mom, next_mom = get_feature_pre(std_dic['mom'])\n",
    "#     trix, next_trix = get_feature_pre(std_dic['trix'])\n",
    "#     print(len(std_dic['etf_close']), ' ', len(close_diff_1), ' ', len(close_diff_2), ' ' , len(rsi))\n",
    "    \n",
    "    rtn_pd = pd.DataFrame({\"etf_id\": etfid, \"idx\": etf_idx_dic[etfid], \"etf_date\": tmp_pd[\"etf_date\"], \n",
    "                           \"etf_close\": std_dic[\"etf_close\"], \"close_dif\": std_dic['close_dif'], \n",
    "                           'ema3': ema3, \"ema5\": ema5, \"ema10\": ema10, \"ema20\": ema20, \"ema30\" : ema30,\n",
    "                          \"ema3_slope\": ema3_slope, \"ema5_slope\": ema5_slope, \"ema10_slope\": ema10_slope,\n",
    "                           \"nbias3\": nbias3, \"nbias6\": nbias6, \"nbias20\": nbias20, \"bias_ma\": bias_ma,\n",
    "                          \"bias_3_6\": bias_3_6, \"k\": k, \"d\": d, \"dif\": dif,\n",
    "                          #\"macd\": macd, \"macdsignal\": macdsignal, \"macdhist\": macdhist,\n",
    "                           \"macd1\": macd1, \"macdsignal1\": macdsignal1, \"macdhist1\": macdhist1,\n",
    "                          \"rsi\": rsi, \"cmo\": cmo, \"rocr\": rocr, \"ppo\": ppo}).dropna(how='any')\n",
    "    #col = 'close_dif'\n",
    "    #rtn_pd = rtn_pd[np.abs(rtn_pd[col] - rtn_pd[col].mean()) <= 3.0*rtn_pd[col].std()] #只取3倍以內標準差資料\n",
    "    \n",
    "    next_pd = pd.DataFrame({\"etf_id\": etfid, \"idx\": etf_idx_dic[etfid], \"etf_date\": next_date, \n",
    "                            'etf_close': -1.0, \"close_dif\": -1.0, \n",
    "                           \"ema3\": next_ema3, \"ema5\": next_ema5, \"ema10\": next_ema10, \"ema20\": next_ema20, \"ema30\" : next_ema30,\n",
    "                          \"ema3_slope\": next_ema3_slope, \"ema5_slope\": next_ema5_slope, \"ema10_slope\": next_ema10_slope,\n",
    "                            \"nbias3\": next_nbias3, \"nbias6\": next_nbias6, \"nbias20\": next_nbias20, \"bias_ma\": next_bias_ma,\n",
    "                          \"bias_3_6\": next_bias_3_6, \"k\": next_k, \"d\": next_d, \"dif\": next_dif,\n",
    "                          #\"macd\": next_macd, \"macdsignal\": next_macdsignal, \"macdhist\": next_macdhist,\n",
    "                            \"macd1\": next_macd1, \"macdsignal1\": next_macdsignal1, \"macdhist1\": next_macdhist1,\n",
    "                          \"rsi\": next_rsi, \"cmo\": next_cmo, \"rocr\": next_rocr, \"ppo\": next_ppo})\n",
    "    \n",
    "    return (rtn_pd, next_pd, scaler_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.41707893989108"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = etf_dic[\"0050\"][[\"etf_close\"]]\n",
    "# print(val[\"etf_close\"].tolist())\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import itertools\n",
    "scaler = StandardScaler()\n",
    "scaler.fit_transform(val)\n",
    "#merged = list(itertools.chain.from_iterable(scaler.fit_transform(val)))\n",
    "#merged\n",
    "scaler.inverse_transform([-1.44880072])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create feature pandas\n",
    "etf_pd_dic = {}\n",
    "next_pd_dic = {}\n",
    "scaler_dic = {}\n",
    "for etfid in etf_ids:\n",
    "    etf_pd, next_pd, scalers = create_feature(etf_dic[etfid.strip()], predict_start_date, {})\n",
    "    etf_pd_dic.update({etfid.strip() : etf_pd})\n",
    "    next_pd_dic.update({etfid.strip() : next_pd})\n",
    "    scaler_dic.update(scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1080.000000\n",
       "mean       70.045093\n",
       "std         7.961666\n",
       "min        55.800000\n",
       "25%        63.837500\n",
       "50%        68.975000\n",
       "75%        74.962500\n",
       "max        88.300000\n",
       "Name: etf_close, dtype: float64"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etf_pd_dic[\"0050\"]['etf_close'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "#etf_pd_dic[\"0054\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "#etf_pd_dic[\"00692\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import itertools\n",
    "# pd_0050=etf_pd_dic[\"0051\"]\n",
    "# pd_0050=pd_0050[pd_0050.etf_date > '20180101']\n",
    "# scaler = MinMaxScaler()\n",
    "# pd_0050['close_dif']=list(itertools.chain.from_iterable(scaler.fit_transform(pd_0050[['close_dif']])))\n",
    "# # pd_0050[['close_dif']].plot()\n",
    "# for col in list(pd_0050.columns.values): #[\"bias_3_6\", \"ema5_slope\", 'ema10_slope']:\n",
    "#     pd_0050[col]=list(itertools.chain.from_iterable(scaler.fit_transform(pd_0050[[col]])))\n",
    "#     pd_0050[['close_dif',col]].plot()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bias_3_6', 'bias_ma', 'cmo', 'd', 'dif', 'ema10', 'ema10_slope', 'ema20', 'ema3', 'ema30', 'ema3_slope', 'ema5', 'ema5_slope', 'k', 'macd1', 'macdhist1', 'macdsignal1', 'nbias20', 'nbias3', 'nbias6', 'ppo', 'rocr', 'rsi']\n"
     ]
    }
   ],
   "source": [
    "non_feature_list = ['etf_id', 'etf_date', 'close_dif', 'etf_close', 'idx']\n",
    "feature_cols = [col for col in list(etf_pd_dic[\"0050\"].columns.values) if col not in non_feature_list]\n",
    "# feature_cols = ['ema5']\n",
    "print(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dt count: (15891, 26)\n",
      "next_pd_dic count: 18 * (1, 26)\n"
     ]
    }
   ],
   "source": [
    "train_dt_org = pd.concat([etf_pd_dic[etfid.strip()] for etfid in etf_ids])\n",
    "std_dic = {}\n",
    "for col in [c for c in train_dt_org.columns.values if c not in [\"etf_id\", \"etf_date\"]]:\n",
    "        scaler_key = col\n",
    "        if scaler_key in scaler_dic:\n",
    "            scaler = scaler_dic[scaler_key]\n",
    "            merged = list(itertools.chain.from_iterable(scaler.transform(train_dt_org[[col]])))                \n",
    "        else:\n",
    "            scaler = MinMaxScaler()\n",
    "            merged = list(itertools.chain.from_iterable(scaler.fit_transform(train_dt_org[[col]])))\n",
    "            #保留etf_close的scaler在後續使用\n",
    "            scaler_dic.update({scaler_key: scaler})\n",
    "        std_dic.update({col: merged})\n",
    "train_dt = pd.DataFrame(std_dic)\n",
    "print('train_dt count:', train_dt.shape)\n",
    "\n",
    "for etfid in etf_ids:\n",
    "    std_dic = {}\n",
    "    next_pd = next_pd_dic[etfid.strip()]\n",
    "    for col in [c for c in next_pd.columns.values if c not in [\"etf_id\", \"etf_date\"]]:\n",
    "        scaler = scaler_dic[col]\n",
    "        merged = list(itertools.chain.from_iterable(scaler.transform(next_pd[[col]])))\n",
    "        std_dic.update({col: merged})\n",
    "    next_pd_dic.update({etfid.strip() : pd.DataFrame(std_dic)})\n",
    "print('next_pd_dic count:', len(next_pd_dic.keys()), '*' , next_pd_dic['0050'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bias_3_6', 'ema20', 'ema30', 'macd1']"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list('bias_3_6,ema20,ema30,macd1'.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "def trainModel(train_dt, feature_cols, f_type):\n",
    "    train_x1 = train_dt[feature_cols].values\n",
    "    if f_type == '1':\n",
    "        #model for close_dif\n",
    "        train_y1 = train_dt[\"close_dif\"].values\n",
    "        #$$$ parameter\n",
    "        model1 = xgb.XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75, \n",
    "            colsample_bytree=1, max_depth=68, n_jobs=4, min_child_weight=0 )\n",
    "        model1.fit(train_x1, train_y1)\n",
    "        return model1\n",
    "    else:\n",
    "        #model for etf_close\n",
    "        train_y2 = train_dt[\"etf_close\"].values\n",
    "        model2 = xgb.XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75, \n",
    "            colsample_bytree=1, max_depth=68, n_jobs=4, min_child_weight=0 )\n",
    "        model2.fit(train_x1, train_y2)\n",
    "        return model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20180601'"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_0050=etf_dic[\"0050\"]\n",
    "prev_date = pd_0050.loc[~pd_0050[\"etf_date\"].isin(next_date_range)][\"etf_date\"].max()\n",
    "prev_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, DoubleType, IntegerType\n",
    "#計算上或下的值(udf)\n",
    "def judge_up_down_pred_native(curr_price, prev_close_price):\n",
    "    if checkNan(prev_close_price):\n",
    "        return 0.0\n",
    "    else:\n",
    "        prev_price = prev_close_price\n",
    "        if curr_price == prev_price:\n",
    "            return 0.0\n",
    "        elif curr_price > prev_price:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 2.0\n",
    "judge_up_down_pred=udf(judge_up_down_pred_native, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------------+--------+--------+-------+---------+---------+----------+------+\n",
      "| etf_id|etf_date|        etf_name|etf_open|etf_high|etf_low|etf_close|etf_count|prev_close|act_ud|\n",
      "+-------+--------+----------------+--------+--------+-------+---------+---------+----------+------+\n",
      "|00701  |20180604|   國泰臺灣低波動30    |   21.14|   21.24|  21.14|    21.24|    140.0|     21.03|   1.0|\n",
      "|00701  |20180605|   國泰臺灣低波動30    |    21.3|   21.33|  21.27|    21.32|     66.0|     21.24|   1.0|\n",
      "|00701  |20180606|   國泰臺灣低波動30    |   21.36|   21.44|  21.36|    21.43|    163.0|     21.32|   1.0|\n",
      "|00701  |20180607|   國泰臺灣低波動30    |   21.51|   21.53|  21.43|     21.5|     53.0|     21.43|   1.0|\n",
      "|00701  |20180608|   國泰臺灣低波動30    |   21.42|   21.42|  21.34|    21.34|      7.0|      21.5|   2.0|\n",
      "|0051   |20180604|元大中型100         |   34.13|   34.25|  34.13|    34.25|     36.0|     33.94|   1.0|\n",
      "|0051   |20180605|元大中型100         |   34.35|    34.4|   34.3|    34.32|     12.0|     34.25|   1.0|\n",
      "|0051   |20180606|元大中型100         |   34.46|   34.74|  34.46|    34.74|     21.0|     34.32|   1.0|\n",
      "|0051   |20180607|元大中型100         |   34.99|   35.05|  34.78|    34.78|     44.0|     34.74|   1.0|\n",
      "|0051   |20180608|元大中型100         |   34.81|   34.94|  34.69|    34.69|     15.0|     34.78|   2.0|\n",
      "|0057   |20180604|富邦摩台            |    50.1|    50.5|   50.1|     50.5|      3.0|      49.9|   1.0|\n",
      "|0057   |20180605|富邦摩台            |    50.7|   50.75|  50.65|    50.65|      4.0|      50.5|   1.0|\n",
      "|0057   |20180606|富邦摩台            |    50.9|   51.25|   50.9|    51.25|      4.0|     50.65|   1.0|\n",
      "|0057   |20180607|富邦摩台            |    51.4|   51.45|   51.4|     51.4|      3.0|     51.25|   1.0|\n",
      "|0057   |20180608|富邦摩台            |    51.1|    51.1|  50.85|    50.85|     10.0|      51.4|   2.0|\n",
      "|006203 |20180604|元大MSCI台灣        |   38.13|   38.28|  38.13|    38.28|     16.0|     37.79|   1.0|\n",
      "|006203 |20180605|元大MSCI台灣        |   38.48|   38.55|  38.47|    38.47|      5.0|     38.28|   1.0|\n",
      "|006203 |20180606|元大MSCI台灣        |   38.52|   38.73|  38.52|    38.73|      7.0|     38.47|   1.0|\n",
      "|006203 |20180607|元大MSCI台灣        |   38.94|   39.09|  38.94|    38.94|      5.0|     38.73|   1.0|\n",
      "|006203 |20180608|元大MSCI台灣        |   38.98|   38.98|   38.6|     38.6|     42.0|     38.94|   2.0|\n",
      "+-------+--------+----------------+--------+--------+-------+---------+---------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#declare previous row windows\n",
    "wsSpec_etf = Window.partitionBy('etf_id').orderBy('etf_date') #time window for normal case\n",
    "#eval dt for evluate score\n",
    "tetf_dt_eval = tetfp_dt.filter(tetfp_dt.etf_date.isin([prev_date] + next_date_range)) \\\n",
    "    .withColumn(\"prev_close\", lag(tetfp_dt.etf_close).over(wsSpec_etf)) \n",
    "tetf_dt_eval =  tetf_dt_eval.withColumn(\"act_ud\", judge_up_down_pred(tetf_dt_eval.etf_close, tetf_dt_eval.prev_close))\n",
    "tetf_dt_eval = tetf_dt_eval.filter(tetf_dt_eval.etf_date.isin(next_date_range))\n",
    "tetf_dt_eval.cache()\n",
    "tetf_dt_eval.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tetf_pd_eval = tetf_dt_eval.toPandas()\n",
    "# tetf_pd_eval.loc[(tetf_pd_eval['etf_id'] == '006203 ') & (tetf_pd_eval['etf_date'] == '20180522')]['etf_close'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score\n",
    "def doEvaluate(predict_res_final, tetf_pd_eval):\n",
    "    etf_close_list = []\n",
    "    act_ud_list = []\n",
    "    prediction_list = []\n",
    "    pred_ud_list = []\n",
    "    acc_list = []\n",
    "    weights = [0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "    judge_score = 0.0\n",
    "    for row in predict_res_final:\n",
    "        eid = row['etf_id']\n",
    "        edate = row['etf_date']\n",
    "        if edate not in next_date_range:\n",
    "            continue\n",
    "        etf_eval = tetf_pd_eval.loc[(tetf_pd_eval['etf_id'] == eid) & (tetf_pd_eval['etf_date'] == edate)]\n",
    "        etf_close = etf_eval['etf_close'].values[0]\n",
    "        act_ud = etf_eval['act_ud'].values[0]\n",
    "        prediction = row['prediction']\n",
    "        pred_ud = row['pred_ud']\n",
    "        #etf score\n",
    "        eidx = next_date_range.index(edate)\n",
    "        judge_score = judge_score + \\\n",
    "            ((0.5 if pred_ud == act_ud else 0.0) + \\\n",
    "             ((etf_close-abs(prediction-etf_close))/etf_close)*0.5)*weights[eidx]\n",
    "        etf_close_list.append(etf_close)\n",
    "        act_ud_list.append(act_ud)\n",
    "        prediction_list.append(prediction)\n",
    "        pred_ud_list.append(pred_ud)\n",
    "        acc_list.append(1.0 if pred_ud == act_ud else 0.0)\n",
    "    #evaluate accuracy_score\n",
    "    accuracy = accuracy_score(act_ud_list, pred_ud_list, normalize=True)\n",
    "#     print(act_ud_list)\n",
    "#     print(pred_ud_list)\n",
    "    rmse = mean_squared_error(etf_close_list, prediction_list)\n",
    "    return (rmse, accuracy, judge_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['bias_3_6', 'bias_ma', 'close_dif', 'cmo', 'd', 'dif', 'ema10', 'ema10_slope', 'ema20', 'ema3', 'ema30', 'ema3_slope', 'ema5', 'ema5_slope', 'etf_close', 'k', 'macd1', 'macdhist1', 'macdsignal1', 'nbias20', 'nbias3', 'nbias6', 'ppo', 'rocr', 'rsi', 'idx'])"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_7percent(prev_close, prediction):\n",
    "    dif = prediction - prev_close\n",
    "    pct = 0.07\n",
    "    dif_max = prev_close * pct\n",
    "    rtn_val = prediction\n",
    "    if abs(dif) > dif_max:\n",
    "        if dif > 0:\n",
    "            rtn_val = prev_close + dif_max\n",
    "        else:\n",
    "            rtn_val = prev_close - dif_max\n",
    "    return rtn_val\n",
    "        \n",
    "def doPredict1(next_date_range, model1, etf_dic, next_pd_dic, scaler_dic, feature_cols):\n",
    "    predict_res_final_1 = []\n",
    "    dummy_date = '20999999'\n",
    "    for date in next_date_range:\n",
    "        for eid in etf_ids:\n",
    "            #do predict\n",
    "            pred_data = next_pd_dic[eid.strip()][feature_cols].values\n",
    "            pred_res = model1.predict(data=pred_data)\n",
    "#             print(pred_res)\n",
    "\n",
    "            etf_dt = etf_dic[eid.strip()]\n",
    "            prev_dt = etf_dt.loc[(etf_dt[\"etf_date\"] < date) ].iloc[-1]\n",
    "#             print('id of prev_dt: ', eid, ', date: ' ,prev_dt[\"etf_date\"])\n",
    "            prev_close = prev_dt[\"etf_close\"]\n",
    "            scaler = scaler_dic['close_dif']\n",
    "            prediction = prev_close + scaler.inverse_transform(pred_res[0])[0][0]\n",
    "            prediction = max_7percent(prev_close, prediction)\n",
    "            #取出所有預測結果作合併，以進行後續成績計算\n",
    "            predict_res_final_1.append({\n",
    "                'etf_id': eid, 'etf_date': date,\n",
    "                'prediction': prediction, 'pred_ud': judge_up_down_pred_native(prediction, prev_close)\n",
    "            })\n",
    "\n",
    "            if (date != next_date_range[-1]) and (date != dummy_date):\n",
    "                #作出新的next_pd\n",
    "                new_dic = {'etf_id': [eid], 'etf_date': [date], 'etf_name': [''], \n",
    "                           'etf_open': [0.0],  'etf_high': [0.0], 'etf_low': [0.0], 'etf_close': [prediction], 'etf_count': [0.0]}\n",
    "                new_pd = pd.DataFrame(data=new_dic)[['etf_id','etf_date','etf_name', 'etf_open', 'etf_high', \n",
    "                                                          'etf_low', 'etf_close', 'etf_count']]\n",
    "                #print(new_pd)\n",
    "                etf_dt=etf_dt.append(new_pd, ignore_index=True)\n",
    "                next_date = next_date_range[next_date_range.index(date)+1]\n",
    "                etf_pd2, next_pd2, _ = create_feature(etf_dt, next_date, scaler_dic)\n",
    "                etf_dic.update({eid.strip(): etf_dt})\n",
    "                next_pd_dic.update({eid.strip(): next_pd2})\n",
    "    return predict_res_final_1\n",
    "    \n",
    "def doPredict2(next_date_range, model2, etf_dic, next_pd_dic, scaler_dic, feature_cols):\n",
    "    predict_res_final_2 = []\n",
    "    for date in next_date_range:\n",
    "        for eid in etf_ids:\n",
    "            #do predict\n",
    "            pred_data = next_pd_dic[eid.strip()][feature_cols].values\n",
    "            pred_res = model2.predict(data=pred_data)\n",
    "#             print(eid.strip(), '-', date, ': ', pred_res[0], '-', \n",
    "#                   scaler_dic[eid.strip()+'-etf_close'].inverse_transform(pred_res[0])[0][0])\n",
    "\n",
    "            etf_dt = etf_dic[eid.strip()]\n",
    "            prev_dt = etf_dt.loc[(etf_dt[\"etf_date\"] < date) ].iloc[-1]\n",
    "    #         print('id of prev_dt: ', eid, ', date: ' ,prev_dt[\"etf_date\"])\n",
    "            prev_close = prev_dt[\"etf_close\"]\n",
    "            scaler = scaler_dic['etf_close']\n",
    "            prediction = scaler.inverse_transform(pred_res[0])[0][0]\n",
    "            prediction = max_7percent(prev_close, prediction)\n",
    "            #取出所有預測結果作合併，以進行後續成績計算\n",
    "            predict_res_final_2.append({\n",
    "                'etf_id': eid, 'etf_date': date,\n",
    "                'prediction': prediction, 'pred_ud': judge_up_down_pred_native(prediction, prev_close)\n",
    "            })\n",
    "\n",
    "            if date != next_date_range[-1]:\n",
    "                #作出新的next_pd\n",
    "                new_dic = {'etf_id': [eid], 'etf_date': [date], 'etf_name': [''], \n",
    "                           'etf_open': [0.0],  'etf_high': [0.0], 'etf_low': [0.0], 'etf_close': [prediction], 'etf_count': [0.0]}\n",
    "                new_pd = pd.DataFrame(data=new_dic)[['etf_id','etf_date','etf_name', 'etf_open', 'etf_high', \n",
    "                                                          'etf_low', 'etf_close', 'etf_count']]\n",
    "                #print(new_pd)\n",
    "                etf_dt=etf_dt.append(new_pd, ignore_index=True)\n",
    "                next_date = next_date_range[next_date_range.index(date)+1]\n",
    "                etf_pd2, next_pd2, _ = create_feature(etf_dt, next_date, scaler_dic)\n",
    "                etf_dic.update({eid.strip(): etf_dt})\n",
    "                next_pd_dic.update({eid.strip(): next_pd2})\n",
    "    return predict_res_final_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for subset in itertools.combinations(feature_cols, 1):\n",
    "#     print(list(subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dic = {\n",
    "    'ema5,dif':'1',\n",
    "    'rocr,ema5':'2',\n",
    "    'ema20,d':'1',\n",
    "    'ema5,k':'2',\n",
    "    'ema5_slope,k':'1',\n",
    "    'ema5_slope,ema10':'1',\n",
    "    'ema20,nbias6':'2',\n",
    "    'ema30,d':'1',\n",
    "    'dif,ema3':'1',\n",
    "    'ema20,rocr':'2',\n",
    "    'nbias3,ema20':'2',\n",
    "    'nbias6,ema3_slope':'1',\n",
    "    'cmo,bias_3_6':'1',\n",
    "    'ema5,rsi':'2',\n",
    "    'ema3,k':'1'\n",
    "}\n",
    "\n",
    "feature_dic = {\n",
    "    'ema20,nbias6,macd1':'2',\n",
    "    'dif,ema30,nbias6':'2',\n",
    "    'ema20,dif,ema5_slope':'2',\n",
    "    'ema3,k,d':'2',\n",
    "    'dif,ema3_slope,ema10':'2',\n",
    "    'ema5,dif,nbias3':'1',\n",
    "    'ema20,d,macdsignal1':'2',\n",
    "    'cmo,bias_3_6,macd1':'1'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ema20,nbias6,macd1 : final2= 14.898895412857804\n",
      "dif,ema30,nbias6 : final2= 14.889072084305246\n",
      "ema20,dif,ema5_slope : final2= 14.844486638603959\n",
      "ema3,k,d : final2= 14.797666048558014\n",
      "dif,ema3_slope,ema10 : final2= 14.603483468527063\n",
      "ema5,dif,nbias3 : final1= 14.57081352054886\n"
     ]
    }
   ],
   "source": [
    "type_list = []\n",
    "feature_list = []\n",
    "acc_list = []\n",
    "rmse_list = []\n",
    "escore_list = []\n",
    "all_pred_list = []\n",
    "\n",
    "# for features in itertools.combinations(feature_cols, feature_num):\n",
    "for feature_str in feature_dic.keys():    \n",
    "    f_type = feature_dic[feature_str]\n",
    "    feature_cols = list(feature_str.split(','))\n",
    "    model = trainModel(train_dt, feature_cols, f_type)\n",
    "    #recover to re-run\n",
    "    for eid in etf_ids:\n",
    "        etf_dt = etf_dic[eid.strip()]\n",
    "        etf_dt = etf_dt.loc[~etf_dt[\"etf_date\"].isin(next_date_range)]\n",
    "        etf_dic.update({eid.strip(): etf_dt})\n",
    "        etf_pd, next_pd, _ = create_feature(etf_dic[eid.strip()], predict_start_date, scaler_dic)\n",
    "#         print(eid.strip(), ': ', etf_pd.shape)\n",
    "        etf_pd_dic.update({eid.strip() : etf_pd})\n",
    "        next_pd_dic.update({eid.strip() : next_pd})\n",
    "    if f_type == '1':\n",
    "        #evaluate close_dif\n",
    "        predict_res_pd_1 = doPredict1(next_date_range, model, etf_dic, next_pd_dic, scaler_dic, feature_cols)\n",
    "        all_pred_list = all_pred_list + predict_res_pd_1\n",
    "        if DoEval:\n",
    "            rmse1, acc1, e_score1 = doEvaluate(predict_res_pd_1, tetf_pd_eval)\n",
    "            type_list.append(\"1\")\n",
    "            feature_list.append(feature_str)\n",
    "            acc_list.append(acc1)\n",
    "            rmse_list.append(rmse1)\n",
    "            escore_list.append(e_score1)\n",
    "        if DoEval:\n",
    "            print(feature_str, ': final1=', e_score1)\n",
    "    else:\n",
    "        #evaluate etf_close\n",
    "        predict_res_pd_2 = doPredict2(next_date_range, model, etf_dic, next_pd_dic, scaler_dic, feature_cols)\n",
    "        all_pred_list = all_pred_list + predict_res_pd_2\n",
    "        if DoEval:\n",
    "            rmse2, acc2, e_score2 = doEvaluate(predict_res_pd_2, tetf_pd_eval)\n",
    "            type_list.append(\"2\")\n",
    "            feature_list.append(feature_str)\n",
    "            acc_list.append(acc2)\n",
    "            rmse_list.append(rmse2)\n",
    "            escore_list.append(e_score2)\n",
    "        if DoEval:\n",
    "            print(feature_str, ': final2=', e_score2)\n",
    "\n",
    "score_dic = {\n",
    "    'feature_type' : type_list, 'feature': feature_list, 'accuracy': acc_list, 'rmse': rmse_list,\n",
    "    'final_score': escore_list\n",
    "}\n",
    "if DoEval:\n",
    "    score_dic = {\n",
    "        'feature_type' : type_list, 'feature': feature_list, 'accuracy': acc_list, 'rmse': rmse_list,\n",
    "        'final_score': escore_list\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# etf_dic['0050']\n",
    "# score_dic['feature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_pd = pd.DataFrame(data=score_dic)[['feature_type', 'feature', 'accuracy', 'rmse', 'final_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "score_pd.sort_values(by=['final_score'], ascending=[0]).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_pred_pd = pd.DataFrame(all_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_ud(ud_list):\n",
    "    num_1 = 0\n",
    "    num_0 = 0\n",
    "    num_2 = 0\n",
    "    for ud in ud_list:\n",
    "        if ud == 1.0:\n",
    "            num_1 += 1\n",
    "        elif ud == 2.0:\n",
    "            num_2 += 1\n",
    "        else:\n",
    "            num_0 += 1\n",
    "    if num_2 >= num_1 and num_2 >= num_0:\n",
    "        return 2.0\n",
    "    if num_1 >= num_2 and num_1 >= num_0:\n",
    "        return 1.0\n",
    "    return 0.0\n",
    "    \n",
    "predict_res_final = []\n",
    "for date in next_date_range:\n",
    "    for eid in etf_ids:\n",
    "        pred_pd = all_pred_pd.loc[(all_pred_pd['etf_id'] == eid) & (all_pred_pd['etf_date'] == date)]\n",
    "        ud_list = pred_pd['pred_ud'].tolist()\n",
    "        ud = judge_ud(ud_list)\n",
    "        close_list = pred_pd[\"prediction\"].tolist()\n",
    "        close = avg_list(close_list)\n",
    "        predict_res_final.append({\n",
    "            'etf_id': eid, 'etf_date': date,\n",
    "            'prediction': close, 'pred_ud': ud\n",
    "        })\n",
    "\n",
    "if DoEval:\n",
    "    rmse, acc, e_score = doEvaluate(predict_res_final, tetf_pd_eval)\n",
    "    print(predict_start_date, ' final: rmse=', rmse, ', accuracy=', acc, 'etf_score=', e_score)\n",
    "# 20180604  final: rmse= 0.35411602595914005 , accuracy= 0.7666666666666667 etf_score= 15.684756862674414\n",
    "# 20180528  final: rmse= 0.21377914756013033 , accuracy= 0.4444444444444444 etf_score= 12.73777330635146\n",
    "# 20180521  final: rmse= 0.4968230593196396 , accuracy= 0.45555555555555555 etf_score= 13.644212655410296\n",
    "# 20180514  final: rmse= 0.3106272364914174 , accuracy= 0.6 etf_score= 14.756825799674976\n",
    "# 20180507  final: rmse= 0.4034365973828622 , accuracy= 0.5111111111111111 etf_score= 13.246689204250412"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_res = spark.createDataFrame(pd.DataFrame(predict_res_final)).orderBy(\"etf_id\", \"etf_date\").collect()\n",
    "#export to pandas\n",
    "etf_ids = []\n",
    "mon_ud = []\n",
    "mon_price = [] \n",
    "tue_ud = []\n",
    "tue_price = []\n",
    "wed_ud = []\n",
    "wed_price = []\n",
    "thu_ud = []\n",
    "thu_price = []\n",
    "fri_ud = []\n",
    "fri_price = []\n",
    "\n",
    "def encode_ud(oper_ud):\n",
    "    if oper_ud == 0.0:\n",
    "        return 0\n",
    "    elif oper_ud == 1.0:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "for row in final_res:\n",
    "    etf_id = row[\"etf_id\"]\n",
    "    if etf_id not in etf_ids:\n",
    "        etf_ids.append(etf_id)\n",
    "    etf_date = row[\"etf_date\"]\n",
    "    eidx = next_date_range.index(row[\"etf_date\"])\n",
    "    if eidx == 0:\n",
    "        mon_ud.append(encode_ud(row[\"pred_ud\"]))\n",
    "        mon_price.append(row[\"prediction\"])\n",
    "    elif eidx == 1:\n",
    "        tue_ud.append(encode_ud(row[\"pred_ud\"]))\n",
    "        tue_price.append(row[\"prediction\"])\n",
    "    elif eidx == 2:\n",
    "        wed_ud.append(encode_ud(row[\"pred_ud\"]))\n",
    "        wed_price.append(row[\"prediction\"])\n",
    "    elif eidx == 3:\n",
    "        thu_ud.append(encode_ud(row[\"pred_ud\"]))\n",
    "        thu_price.append(row[\"prediction\"])\n",
    "    elif eidx == 4:\n",
    "        fri_ud.append(encode_ud(row[\"pred_ud\"]))\n",
    "        fri_price.append(row[\"prediction\"])\n",
    "        \n",
    "if len(mon_ud) == 0:\n",
    "    mon_ud = list(0 for i in range(0,len(etf_ids)))\n",
    "    mon_price = list(0.0 for i in range(0,len(etf_ids)))\n",
    "if len(tue_ud) == 0:\n",
    "    tue_ud = list(0 for i in range(0,len(etf_ids)))\n",
    "    tue_price = list(0.0 for i in range(0,len(etf_ids)))\n",
    "if len(wed_ud) == 0:\n",
    "    wed_ud = list(0 for i in range(0,len(etf_ids)))\n",
    "    wed_price = list(0.0 for i in range(0,len(etf_ids)))\n",
    "if len(thu_ud) == 0:\n",
    "    thu_ud = list(0 for i in range(0,len(etf_ids)))\n",
    "    thu_price = list(0.0 for i in range(0,len(etf_ids)))\n",
    "if len(fri_ud) == 0:\n",
    "    fri_ud = list(0 for i in range(0,len(etf_ids)))\n",
    "    fri_price = list(0.0 for i in range(0,len(etf_ids)))\n",
    "    \n",
    "dic = {\"ETFid\": etf_ids, \n",
    "       \"Mon_ud\": mon_ud, \"Mon_cprice\": mon_price,\n",
    "       \"Tue_ud\": tue_ud, \"Tue_cprice\": tue_price,\n",
    "       \"Wed_ud\": wed_ud, \"Wed_cprice\": wed_price,\n",
    "       \"Thu_ud\": thu_ud, \"Thu_cprice\": thu_price,\n",
    "       \"Fri_ud\": fri_ud, \"Fri_cprice\": fri_price\n",
    "      }\n",
    "final_df = pd.DataFrame(data=dic)[['ETFid','Mon_ud','Mon_cprice','Tue_ud','Tue_cprice',\n",
    "                                  'Wed_ud','Wed_cprice','Thu_ud','Thu_cprice',\n",
    "                                  'Fri_ud','Fri_cprice']]\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(Path.replace(\"file:\",\"\") + \"/etf_price_pred_talib_eval_mac.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
